\capitulo{4}{Técnicas y herramientas}

\section{Técnicas}

Scrum???

\subsection{CRISP-DM}
Una metodología común, no propietaria y abierta de desarrollo para proyectos de ML es CRISP-DM (Cross-Industry Standard Process for Data Mining). Nacida del trabajo conjunto de un grupo de empresas, consta de un proceso iterativo y adaptable.

Sin pretender desarrollar exhaustivamente la misma, se muestra a continuación un resumen de sus 6 principales fases~\cite{apuntesSisint}.

\begin{enumerate}
	\item \textbf{Compresión del negocio:} el objetivo de esta fase es entender los requerimientos del proyecto observados bajo un prisma empresarial y aplicar dicho conocimiento a la definición del problema de minería de datos.
	
	\item \textbf{Compresión de los datos:} consiste en recopilar y explorar los datos con el objetivo de conocerlos. También se pretende identificar problemas de calidad y posibles subconjuntos de datos con especial interés para ser analizados.
	
	\item \textbf{Preparación de los datos:} se busca procesar el conjunto de datos seleccionado con el fin de ser utilizable en algoritmos de ML. Por lo tanto, se incluyen todas las tareas de adecuación de los datos al problema de aprendizaje que se consideren necesarios.
	
	\item \textbf{Modelado:} en esta fase se pretende aplicar las técnicas de ML seleccionadas a las vistas obtenidas anteriormente, y se evalúa utilizando técnicas específicas de aprendizaje. 
	
	\item \textbf{Evaluación:} a diferencia de la fase anterior, se busca evaluar el modelo desde el punto de vista del objetivo del negocio. Es decir, determinar si responde ante los objetivos empresariales planteados con anterioridad.
	
	\item \textbf{Despliegue y explotación:} en la última fase, se integran los modelos en los procesos de la organización y se aprovecha el conocimiento generado.
\end{enumerate}

Es relevante destacar la naturaleza cíclica y no rígida de la metodología, permitiendo que algunas fases alimenten sus predecesoras o que incluso algunos proyectos motiven el lanzamiento de otros.

\section{Herramientas}

Se muestra a continuación algunas de las herramientas más relevantes en el desarrollo del proyecto.

\subsection{Librerías}

\subsubsection{Scikit-Learn}

Una de las librerías referentes en el ámbito de \textit{machine learning} y Python. Además de utilizar la vertiente relacionada con aprendizaje automático, también se aprovecharon otras ramas del módulo como la de extracción de características de texto (TF-IDF)~\cite{sslearnRepo}. Debido a que proporciona una interfaz estándar para los clasificadores base de muchos algoritmos, posee muy buena documentación y es compatible con otras librerías, ha sido muy utilizada.

\subsubsection{sslearn}

Librería desarrollada por José Luis Garrido-Labrador dedicada al aprendizaje semisupervisado en Python. Utilizada para realizar comparaciones con algunos de los algoritmos implementados~\cite{sslearnRepo}.

\subsubsection{LAMDA}

\textit{Toolkit} escrito en Python con algunas implementaciones de los algoritmos más relevantes de aprendizaje semisupervisado~\cite{lamdasslRepo}. Nuevamente, ha sido utilizada como referente a la hora de realizar comparaciones contra las implementaciones propias~\cite{lamdasslPaper}.

\subsubsection{Beautiful soup}

Biblioteca de Python utilizada para extraer datos de ficheros \texttt{HTML} obtenidos mediante la realización de \textit{web scraping} a la hora de sintetizar vectores de características para la detección de \textit{phishing}. Se ha recurrido a ella, principalmente, cuando la obtención de ciertos campos en las etiquetas puede ser vulnerada mediante el uso de expresiones regulares corrientes~\cite{bs4Docs}.

\subsubsection{TLD}

Librería de Python utilizada para extraer dominios de alto nivel (TLDs) de los enlaces facilitados~\cite{tldLibreria}. Para evitar dependencias de biblitecas externas, en un primer momento se consideró trabajar con una lista del \textit{top} dominios más comunes (extraída de~\cite{tldLista}). Sin embargo, se descartó la idea debido a que no existe una forma intrínseca de saber qué cadenas son TLD y cuales son subdominios. Por ejemplo, \texttt{zap.co.it} es un subdominio ya que existe el TLD \texttt{co.it}. Sin embargo, en un país que no venda dominios de la forma \texttt{co.<<pais>>} sería un TLD~\cite{tldNogenerico}.

\subsubsection{NLTK}

\textit{Toolkit} de Python utilizado para trabajar con lenguaje natural. Su principal utilidad ha sido procesar palabras como \textit{tokens} para poder facilitar la implementación de algoritmos más complejos como TD-IDF o para analizar y procesar texto~\cite{nltk}.

\subsubsection{Otros}

Otras de las dependencias (estándar) del proyecto se enumeran a continuación.

\begin{itemize}
	\item \textbf{Requests}: permite realizar peticiones a distintas URLs, además de especificar parámetros relevantes en las mismas (como \textit{headers}, \textit{cookies}, \textit{proxies} o \textit{timeouts}).
	\item \textbf{urllib}: procesa URLs y las divide en campos.
	\item \textbf{re}: utilizada para aplicar expresiones regulares en Python.
	\item \textbf{Numpy, Pandas, Matplotlib}: utilizadas para tratar vectores, operaciones en \textit{arrays}, \textit{dataframes} y representar resultados.
\end{itemize}


\subsection{Extensiones y portales}

\subsubsection{ZenHub}

Zenhub es una extensión dedicada a la gestión de proyectos \textit{software} que se integra directamente con Github. Permite visualizar proyectos, \textit{sprints}, gráficos propios de Scrum y crear tableros. Debido a que el equipo de desarrollo cuenta con un único integrante, se considera la alternativa óptima (por ejemplo, a Jira) por su sencillez~\cite{zenhubHome}.

\subsubsection{Github}

Portal que permite alojar distintos repositorios en la nube~\cite{githubHome}. Ha sido escogido por ser una de las plataformas más populares, además de por permitir la interacción entre distintos usuarios. La gestión del \textit{backlog} del producto ha sido simulada mediante la creación de \textit{issues}.


\subsection{Programas}

\subsubsection{KEEL}

Herramienta desarrollada en Java por distintas universidades españolas y financiada por el Ministerio de Educación y Ciencia~\cite{keelRepo}. Proporciona implementaciones de \textit{machine learning}, y ha sido utilizada para probar aquellos algoritmos no disponibles en las librerías de Python mencionadas anteriormente.

\subsection{Entorno de programación}

\begin{itemize}
	\item \textbf{Python}: lenguaje de programación interpretado de propósito general. Ha sido escogido para realizar este proyecto debido a la gran cantidad de paquetes dedicados a la ciencia de datos que posee~\cite{python}.
	\item \textbf{Visual Studio Code}: editor y depurador de código empleado junto a algunas de sus principales extensiones.
	\item \textbf{Entornos virtuales}: un entorno virtual es un directorio que contiene una instalación concreta de Python y los paquetes que se hayan decidido instalar~\cite{venvs}. Como se puede deducir, la correcta utilización de los entornos virtuales conlleva muchas ventajas, como tener varios paquetes sin conflictos entre ellos o evitar corromper la instalación base si se está probando algún elemento nuevo.
	\item \textbf{Conda}: es un sistema de gestión de paquetes y entornos \textit{opensource}. Es especialmente útil a la hora de realizar trabajos de ML ya que posee librerías como SciPy y TensorFlow~\cite{conda}.
\end{itemize}

\subsection{Control de calidad}

\begin{itemize}
	\item \textbf{SonarCloud}: se trata de un servicio de análisis estático de código que notifica diversos parámetros a revisar como \textit{codesmells}, \textit{bugs} o vulnerabilidades. Se ha integrado en el repositorio con la intención de realizar código limpio y aumentar la calidad del proyecto~\cite{sonarCloud}.
	\item \textbf{DeepSource}: se trata de una plataforma que fusiona el análisis estático de código, SAST (\textit{static application security testing}), cobertura de código y más para mejorar la calidad de los proyectos. Además, posee un \textit{bot} que realiza ciertas correcciones automáticas (por ejemplo, modifica errores de documentación) y facilita \textit{badges} para los repositorios~\cite{deepSourceBot}.
	\item \textbf{Travis CI}: se ha utilizado el servicio para la realización automática de \textit{tests} cada vez que se actualiza el repositorio. Para ello, se ha configurado una \textit{build} y recreado el entorno del programador. De este modo, antes de realizar un \textit{merge} a la rama principal se puede comprobar rápidamente si las pruebas siguen pasando, lo que mejora la integración y despliegue continuo~\cite{travisCI}.
\end{itemize}

\subsection{Otros}

\begin{itemize}
	\item \textbf{\TeX{}Studio}: editor de \LaTeX{} utilizado.
	\item \textbf{Git BASH}: emulador de BASH para Microsoft Windows que proporciona una terminal de línea de comandos de Git.
\end{itemize}


\subsection{\textit{Scripts}} 
\subsubsection{\textit{Script} para levantar proxies SOCKS5}
\label{sec:script_tor}
Durante la extracción de vectores de características, se realizan peticiones a páginas de \textit{phishing}. Para garantizar que estas páginas no puedan rastrear desde donde se ha realizado la petición, se han utilizado \textit{proxies} que implementan el protocolo SOCKS5.

Para ello, se ha implementado en python un \textit{script} auxiliar que levanta en paralelo tantas instancias de Tor como se soliciten, y mantiene los hilos vivos hasta que se interrumpa la ejecución del \textit{script}.

Por cada instancia de Tor que se quiera levantar, se necesita un fichero \texttt{torrc} en el directorio \texttt{/etc/tor/}~\cite{TorFicherosTor}. Cada una de ellas debe tener, además, su propio puerto de control, su propio puerto \textit{socks} y su directorio de datos. Por ello, se ha creado una clase auxiliar que genera estos ficheros. Para levantar la instancia simplemente ha de ejecutarse el comando \texttt{tor -f /etc/tor/torrc.x} (siendo $x$ el número de la instacia correspondiente), aunque se ha decidido, además, dirigir la salida al fichero \texttt{/dev/null}. Es importante destacar que el puerto de control debe ser el siguiente al puerto \textit{socks}. Teniendo en cuenta que los puertos por defecto de Tor son el 9050 y el 9051, se puede incrementar partiendo de esos números~\cite{TorficheroComando}. Para comprobar que la instancia levantada funciona correctamente, se hace una petición a \url{http://ipinfo.io/ip} y se comprueba con una expresión regular que la dirección obtenida es la correcta. De este modo, se sabe que el \textit{proxy} HTTP levantado funciona, y se puede utilizar redireccionando las peticiones oportunas a través del \textit{proxy} \texttt{socks5h://127.0.0.1:$y$} (donde $y$ es el puerto \textit{socks} de la instancia correspondiente). Para que pueda ser usado en código, se ha generado un \texttt{JSON} donde se incluyen como diccionarios los \textit{proxies} disponibles, y se utilizan en combinación con la librería Requests.

Debido a que el puerto Tor queda <<a la escucha>> en la máquina local (está esperando que se realice una conexión), no conlleva ningún riesgo. Para cerrar el puerto, es suficiente con parar el proceso que esté ejecutando el \textit{script}. Se puede comprobar ejecutando en una terminal el comando \texttt{sudo lsof -i:$y$} (nuevamente, $y$ es el puerto \textit{socks} levantado). Cuando el \textit{script} esté funcionando, la salida del comando mostrará diversos campos, como el propio comando (Tor), el PID, el nombre (\textit{listen}). Si el \textit{script} se para, el comando no mostrará salida, lo que implica que el puerto no está abierto~\cite{checkOpenTorPorts}.