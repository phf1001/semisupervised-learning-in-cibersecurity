{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PHISHING VECTOR GENERATOR** üêü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from os import path\n",
    "from tld import get_tld\n",
    "from bs4 import BeautifulSoup\n",
    "from phishing_utils import *\n",
    "from user_browsing import user_browsing\n",
    "from difflib import SequenceMatcher\n",
    "from html import unescape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PHISH_FVG:\n",
    "    def __init__(self, url, tfidf):\n",
    "        self.url = url\n",
    "        parsed = urlparse(url)\n",
    "        self.base = parsed.netloc\n",
    "        self.path = parsed.path\n",
    "\n",
    "        self.fv = np.array([-1 for i in range(19)])\n",
    "\n",
    "        self.user = user_browsing()\n",
    "\n",
    "        response_content = get_bin_source_code(\n",
    "            self.url, self.user.get_simple_user_header_agent(), self.user.proxies\n",
    "        )\n",
    "\n",
    "        content = response_content.decode(\"utf-8\", errors=\"ignore\")\n",
    "        self.html = unescape(content)\n",
    "        self.soup = BeautifulSoup(response_content)\n",
    "\n",
    "        self.hyperlinks = find_hyperlinks_tags(self.soup)\n",
    "        self.tfidf = tfidf\n",
    "\n",
    "        self.extra_information = {\"f{}\".format(i): None for i in range(1, 20)}\n",
    "\n",
    "    def set_feature_vector(self):\n",
    "        self.set_f1()\n",
    "        self.set_f2()\n",
    "        self.set_f3()\n",
    "        self.set_f4()\n",
    "        self.set_f5()\n",
    "        self.set_f6()\n",
    "        self.set_f7()\n",
    "        self.set_f8()\n",
    "        self.set_f9()\n",
    "        self.set_f10_f11()\n",
    "        self.set_f12()\n",
    "        self.set_f13()\n",
    "        self.set_f14()\n",
    "        self.set_f15()\n",
    "        self.set_f16()\n",
    "        self.set_f17()\n",
    "        self.set_f18()\n",
    "        self.set_f19()\n",
    "\n",
    "    def set_f1(self):\n",
    "        \"\"\"\n",
    "        Sets F1.\n",
    "        F1 = 1, if dots in url >= 4\n",
    "        F1 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        n_dots = self.url.count(\".\")\n",
    "        self.extra_information[\"f1\"] = n_dots\n",
    "\n",
    "        if n_dots >= 4:\n",
    "            self.fv[0] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[0] = 0\n",
    "\n",
    "    def set_f2(self):\n",
    "        \"\"\"\n",
    "        Sets F2.\n",
    "        F2 = 1, if URL contains '@' or '-' symbols\n",
    "        F2 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        at_found = \"@\" in self.url\n",
    "        minus_found = \"-\" in self.url\n",
    "\n",
    "        found_characters = \"\"\n",
    "\n",
    "        if at_found and minus_found:\n",
    "            found_characters += \"@ y -\"\n",
    "        elif at_found:\n",
    "            found_characters += \"@\"\n",
    "        elif minus_found:\n",
    "            found_characters += \"-\"\n",
    "        else:\n",
    "            found_characters += \"ninguno\"\n",
    "\n",
    "\n",
    "        self.extra_information[\"f2\"] = found_characters\n",
    "\n",
    "        if at_found or minus_found:\n",
    "            self.fv[1] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[1] = 0\n",
    "\n",
    "    def set_f3(self):\n",
    "        \"\"\"\n",
    "        Sets F3.\n",
    "        F3 = 1, if URL length >= 74\n",
    "        F3 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        self.extra_information[\"f3\"] = len(self.url)\n",
    "\n",
    "        if len(self.url) >= 74:\n",
    "            self.fv[2] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[2] = 0\n",
    "\n",
    "    def set_f4(self):\n",
    "        \"\"\"\n",
    "        Sets F4.\n",
    "        F4 = 1, if URL contains any suspicious word\n",
    "        F4 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        splitted_url = get_splitted_url(self.url)\n",
    "        suspicious_words = get_suspicious_keywords()\n",
    "\n",
    "        for word in splitted_url:\n",
    "            leet_translation = translate_leet_to_letters(word)  # Decisi√≥n propia\n",
    "\n",
    "            if bool(suspicious_words & leet_translation):\n",
    "                self.fv[3] = 1\n",
    "                self.extra_information[\"f4\"] = word\n",
    "                return\n",
    "\n",
    "        self.fv[3] = 0\n",
    "        self.extra_information[\"f4\"] = 'ninguna'\n",
    "\n",
    "    def set_f5(self):\n",
    "        \"\"\"\n",
    "        Sets F5.\n",
    "        F5 = 1, if tlds in URL > 1\n",
    "        F5 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        tld = get_tld(self.url, fix_protocol=True)\n",
    "        base_without_tld = self.base[: -len(tld) - 1]\n",
    "        rest = base_without_tld + self.path\n",
    "\n",
    "        splitted_url = get_splitted_url_keep_dots(rest)\n",
    "        tlds = get_tlds_set()\n",
    "        tlds = set([\".\" + tld for tld in tlds])\n",
    "\n",
    "        extra_tlds_found = splitted_url & tlds\n",
    "\n",
    "        if len(extra_tlds_found) >= 1:\n",
    "            self.fv[4] = 1\n",
    "            self.extra_information[\"f5\"] = ', '.join(extra_tlds_found)\n",
    "\n",
    "        else:\n",
    "            self.fv[4] = 0\n",
    "            self.extra_information[\"f5\"] = 'ninguno'\n",
    "\n",
    "    def set_f6(self):\n",
    "        \"\"\"\n",
    "        Sets F6.\n",
    "        F6 = 1, if http count in URL > 1\n",
    "        F6 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        if len(re.findall(\"http\", self.url)) > 1:\n",
    "            self.fv[5] = 1\n",
    "            self.extra_information[\"f6\"] = 'S√≠'\n",
    "\n",
    "        else:\n",
    "            self.fv[5] = 0\n",
    "            self.extra_information[\"f6\"] = 'No'\n",
    "\n",
    "\n",
    "    def set_f7(self):\n",
    "        \"\"\"\n",
    "        Sets F7.\n",
    "        F7 = 1, if brand in incorrect position.\n",
    "        F7 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        targets = get_phishing_targets_set()\n",
    "\n",
    "        lower_url = self.url.lower()\n",
    "        parsed = urlparse(lower_url)\n",
    "        path = parsed.path\n",
    "\n",
    "        for target in targets:\n",
    "            if target in path:  # or target in sub_domains\n",
    "                self.fv[6] = 1\n",
    "                self.extra_information[\"f7\"] = target\n",
    "                return\n",
    "\n",
    "        # Extra checking - leet translation wherever except base\n",
    "        # domain and tld but not exact word\n",
    "\n",
    "        base = parsed.netloc\n",
    "        tld = get_tld(lower_url, fix_protocol=True)\n",
    "        without_tld = base[: -len(tld) - 1]\n",
    "\n",
    "        if without_tld.count(\".\") > 0:\n",
    "            sub_domains = without_tld[: without_tld.rindex(\".\")]\n",
    "        else:\n",
    "            sub_domains = without_tld\n",
    "\n",
    "        for word in get_splitted_url(path + sub_domains):\n",
    "            leet_translation = translate_leet_to_letters(word)  # Decisi√≥n propia\n",
    "\n",
    "            # If the original one does not have numbers it is removed\n",
    "            if not re.search(r\"\\d\", word):\n",
    "                leet_translation -= {word}\n",
    "\n",
    "            for fake in leet_translation:\n",
    "                for target in targets:\n",
    "                    if SequenceMatcher(None, fake, target).ratio() >= 0.8:\n",
    "                        self.fv[6] = 1\n",
    "                        self.extra_information[\"f7\"] = target\n",
    "                        return\n",
    "\n",
    "        self.fv[6] = 0\n",
    "        self.extra_information[\"f7\"] = 'ninguna'\n",
    "\n",
    "    def set_f8(self):\n",
    "        \"\"\"\n",
    "        Sets F8.\n",
    "        F8 = 1, if data URI present in website.\n",
    "        F8 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        matches = find_data_URIs(self.html)\n",
    "\n",
    "        if len(matches) > 0:\n",
    "            self.fv[7] = 1\n",
    "            self.extra_information[\"f8\"] = len(matches) \n",
    "\n",
    "        else:\n",
    "            self.fv[7] = 0\n",
    "            self.extra_information[\"f8\"] = 0\n",
    "\n",
    "\n",
    "    def set_f9(self):\n",
    "        \"\"\"\n",
    "        Sets F9.\n",
    "        F9 = 1, if action field is blank or javascript:void(0)\n",
    "        F9 = 1, if action field is <name>.php\n",
    "        F9 = 1, if action field contains foreign base domain\n",
    "        F9 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        forms_found = re.findall(\"<form[^>]+>\", self.html)\n",
    "\n",
    "        if len(forms_found) > 0:\n",
    "            for i in range(len(forms_found)):\n",
    "                form_found = forms_found[i]\n",
    "                action_content = re.findall('(?:action=\")([^\"]*)(?:\")', form_found)\n",
    "\n",
    "                if len(action_content) > 0:\n",
    "                    if is_empty(action_content[0]):\n",
    "                        self.fv[8] = 1\n",
    "                        self.extra_information[\"f9\"] = \"vac√≠o, asterisco o _javascript:void(0)_\"\n",
    "                        return\n",
    "\n",
    "                    elif is_simple_php_file(action_content[0]):\n",
    "                        self.fv[8] = 1\n",
    "                        self.extra_information[\"f9\"] = \"Compatible con _fichero.php_\"\n",
    "                        return\n",
    "\n",
    "                    elif is_foreign(self.url, action_content[0]):\n",
    "                        self.fv[8] = 1\n",
    "                        self.extra_information[\"f9\"] = \"Dominio extranjero\"\n",
    "                        return\n",
    "\n",
    "        self.fv[8] = 0\n",
    "        self.extra_information[\"f9\"] = \"No peligroso\"\n",
    "\n",
    "\n",
    "    def set_f10_f11(self):\n",
    "        \"\"\"\n",
    "        Sets F10 and F11.\n",
    "\n",
    "        F10 = number of hyperlinks in source code.\n",
    "\n",
    "        F11 = 1, if no hyperlinks found in source.\n",
    "        F11 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        n_hyperlinks_found = len(self.hyperlinks)\n",
    "        self.fv[9] = n_hyperlinks_found\n",
    "        self.extra_information[\"f10\"] = n_hyperlinks_found\n",
    "\n",
    "        if n_hyperlinks_found == 0:\n",
    "            self.fv[10] = 1\n",
    "            self.extra_information[\"f11\"] = \"S√≠\"\n",
    "\n",
    "        else:\n",
    "            self.fv[10] = 0\n",
    "            self.extra_information[\"f11\"] = \"No\"\n",
    "\n",
    "\n",
    "    def set_f12(self):\n",
    "        \"\"\"\n",
    "        Sets F12.\n",
    "\n",
    "        ratio = |n_foreign_hyp| / |n_hyp|\n",
    "\n",
    "        F12 = 1 if ratio > 0.5 and n_hyp > 0\n",
    "        F12 = 0 otherwise\n",
    "        \"\"\"\n",
    "        n_foreigns = get_number_foreign_hyperlinks(self.url, self.hyperlinks)\n",
    "        self.extra_information[\"f12\"] = n_foreigns\n",
    "        \n",
    "        if len(self.hyperlinks) == 0:\n",
    "            self.fv[11] = 0  # Opino que deber√≠a ser 1\n",
    "            return\n",
    "\n",
    "        ratio = n_foreigns / len(self.hyperlinks)\n",
    "\n",
    "        if ratio > 0.5:\n",
    "            self.fv[11] = 1\n",
    "        else:\n",
    "            self.fv[11] = 0\n",
    "\n",
    "    def set_f13(self):\n",
    "        \"\"\"\n",
    "        Sets F13.\n",
    "\n",
    "        ratio = |n_empty_hyp| / |n_hyp|\n",
    "\n",
    "        F13 = 1 if ratio > 0.34 and n_hyp > 0\n",
    "        F13 = 0 otherwise\n",
    "        \"\"\"\n",
    "        n_empty = get_number_empty_hyperlinks(self.hyperlinks)\n",
    "        self.extra_information[\"f13\"] = n_empty\n",
    "\n",
    "        if len(self.hyperlinks) == 0:\n",
    "            self.fv[12] = 0  # Opino que deber√≠a ser 1\n",
    "            return\n",
    "\n",
    "        ratio = n_empty / len(self.hyperlinks)\n",
    "\n",
    "        if ratio > 0.34:\n",
    "            self.fv[12] = 1\n",
    "        else:\n",
    "            self.fv[12] = 0\n",
    "\n",
    "    def set_f14(self):\n",
    "        \"\"\"\n",
    "        Sets F14.\n",
    "\n",
    "        ratio = |n_errors_hyp| / |n_hyp|\n",
    "\n",
    "        F14 = 1 if ratio > 0.3 and n_hyp > 0\n",
    "        F14 = 0 otherwise\n",
    "        \"\"\"\n",
    "        n_errors = get_number_errors(\n",
    "            self.hyperlinks, self.user.get_simple_user_header_agent(), self.user.proxies\n",
    "        )\n",
    "        self.extra_information[\"f14\"] = n_errors\n",
    "\n",
    "        if len(self.hyperlinks) == 0:\n",
    "            self.fv[13] = 0  # Opino que deber√≠a ser 1\n",
    "            return\n",
    "\n",
    "        ratio = n_errors / len(self.hyperlinks)\n",
    "\n",
    "        if ratio > 0.3:\n",
    "            self.fv[13] = 1\n",
    "        else:\n",
    "            self.fv[13] = 0\n",
    "\n",
    "    def set_f15(self):\n",
    "        \"\"\"\n",
    "        Sets F15.\n",
    "\n",
    "        ratio = |n_redirects| / |n_hyp|\n",
    "\n",
    "        F15 = 1 if ratio > 0.3 and n_hyp > 0\n",
    "        F15 = 0 otherwise\n",
    "        \"\"\"\n",
    "        n_redirects = get_number_redirects(\n",
    "            self.hyperlinks, self.user.get_simple_user_header_agent(), self.user.proxies\n",
    "        )\n",
    "        self.extra_information[\"f15\"] = n_redirects\n",
    "\n",
    "        if len(self.hyperlinks) == 0:\n",
    "            self.fv[14] = 0  # Opino que deber√≠a ser 1\n",
    "            return\n",
    "\n",
    "        ratio = n_redirects / len(self.hyperlinks)\n",
    "\n",
    "        if ratio > 0.3:\n",
    "            self.fv[14] = 1\n",
    "        else:\n",
    "            self.fv[14] = 0\n",
    "\n",
    "    def set_f16(self):\n",
    "        \"\"\"\n",
    "        Sets F16.\n",
    "\n",
    "        F16 = 1, if CSS file is external and contains foreign domain name\n",
    "        F16 = 0, otherwise\n",
    "\n",
    "        #'assets/bootstrap/css/bootstrap.min.css' foreign?\n",
    "        \"\"\"\n",
    "\n",
    "        external_csss = self.soup.findAll(\"link\", rel=\"stylesheet\")\n",
    "\n",
    "        for css in external_csss:\n",
    "            link = extract_url_href(css)\n",
    "\n",
    "            if is_foreign(self.url, link):\n",
    "                self.extra_information[\"f16\"] = link\n",
    "                self.fv[15] = 1\n",
    "                return\n",
    "\n",
    "        self.fv[15] = 0\n",
    "        self.extra_information[\"f16\"] = \"ninguno\"\n",
    "\n",
    "    def set_f17(self):\n",
    "        \"\"\"\n",
    "        Sets F17.\n",
    "        F17 = 0 if copyright keyword matches base domain\n",
    "        F17 = 1, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        copyright_clues = [\n",
    "            \"¬©\",\n",
    "            \"&#169\",\n",
    "            \"& copy\",\n",
    "            \"&copy\",\n",
    "            \"copy\",\n",
    "            \"copyright\",\n",
    "            \"copyright\",\n",
    "            \"all right reserved\",\n",
    "            \"rights\",\n",
    "            \"right\",\n",
    "        ]  # '@',\n",
    "\n",
    "        for clue in copyright_clues:\n",
    "            regex = \"(?:{})([^<.>\\\"']*)(?:[<.>\\\"'])\".format(clue)\n",
    "            copy_contents = re.findall(regex, self.html)\n",
    "\n",
    "            for copy_content in copy_contents:\n",
    "                copy_content = remove_punctuation(copy_content).reshape(1)\n",
    "\n",
    "                for content in copy_content[0].split():\n",
    "                    # Avoid single letters or small strings\n",
    "                    if len(content) > 2 and re.findall(\n",
    "                        content.replace(\",\", \"\"), self.base, re.IGNORECASE\n",
    "                    ):\n",
    "                        self.fv[16] = 0\n",
    "                        self.extra_information[\"f17\"] = content\n",
    "                        return\n",
    "\n",
    "        self.fv[16] = 1\n",
    "        self.extra_information[\"f17\"] = \"ninguna\"\n",
    "\n",
    "    def set_f18(self):\n",
    "        \"\"\"\n",
    "        Set F18.\n",
    "        F18 = 1 if no keyword matches domain name\n",
    "        F18 = 0 Otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        keywords = get_site_keywords(self.html, self.tfidf, 15)\n",
    "\n",
    "        for keyword in keywords:\n",
    "            if re.findall(keyword, self.base):\n",
    "                self.fv[17] = 0\n",
    "                self.extra_information[\"f18\"] = keyword\n",
    "                return\n",
    "\n",
    "        self.fv[17] = 1\n",
    "        self.extra_information[\"f18\"] = \"ninguna\"\n",
    "\n",
    "    def set_f19(self):\n",
    "        \"\"\"\n",
    "        Sets F19.\n",
    "        F19 = 1, if foreign domain found in favicon link\n",
    "        F19 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        icons = self.soup.findAll(\"link\", rel=\"icon\")\n",
    "        icons += self.soup.findAll(\"link\", rel=\"shortcut icon\")\n",
    "\n",
    "        for icon in icons:\n",
    "            link = extract_url_href(icon)\n",
    "\n",
    "            if is_foreign(self.url, link):\n",
    "                self.fv[18] = 1\n",
    "                self.extra_information[\"f19\"] = link\n",
    "                return\n",
    "\n",
    "        self.fv[18] = 0\n",
    "        self.extra_information[\"f19\"] = \"No\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TESTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.895s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "\n",
    "class RealFV(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.ph_entity = PHISH_FVG(\"https://ubuvirtual.ubu.es/\", None)\n",
    "\n",
    "    def test_proxy_working(self):\n",
    "        ip_one = requests.get(\n",
    "            \"http://ipinfo.io/ip\", proxies=self.ph_entity.user.proxies\n",
    "        ).text\n",
    "        ip_two = requests.get(\"http://ipinfo.io/ip\").text\n",
    "        self.assertTrue(ip_one != ip_two)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=[\"first-arg-is-ignored\"], exit=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = user_browsing()\n",
    "\n",
    "urls = get_csv_data(get_data_path() + os.sep + \"alexa_filtered.csv\")[:3]\n",
    "\n",
    "corpus = get_tfidf_corpus(urls, user.get_simple_user_header_agent(), user.proxies)\n",
    "tfidf = get_tfidf(corpus)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GENERATION OF VECTORS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   1   0 190   0   0   0   0   0   0   0   0\n",
      "   0]\n",
      "{'f1': 2, 'f2': '', 'f3': 32, 'f4': 'ninguna', 'f5': 'ninguno', 'f6': 'No', 'f7': 'ninguna', 'f8': 11, 'f9': 'No peligroso', 'f10': 190, 'f11': 'No', 'f12': 41, 'f13': 13, 'f14': 0, 'f15': 3, 'f16': 'ninguno', 'f17': 'Natura', 'f18': 'se', 'f19': 'ninguna'}\n"
     ]
    }
   ],
   "source": [
    "reales = ['https://www.naturaselection.com/']\n",
    "fvs_real = []\n",
    "\n",
    "for real in reales:\n",
    "\n",
    "    try:\n",
    "        ph_entity = PHISH_FVG(real, tfidf)\n",
    "        ph_entity.set_feature_vector()\n",
    "        fvs_real.append(np.append(ph_entity.fv, [0]))\n",
    "        print(ph_entity.fv)\n",
    "        print(ph_entity.extra_information)\n",
    "\n",
    "    except:\n",
    "        print(f\"Error en {real}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **REALS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reales = get_alexa_sites()\n",
    "# fvs_real = []\n",
    "\n",
    "# for real in reales:\n",
    "\n",
    "#     try:\n",
    "#         ph_entity = PHISH_FVG(real, tfidf)\n",
    "#         ph_entity.set_feature_vector()\n",
    "#         fvs_real.append(np.append(ph_entity.fv, [0]))\n",
    "\n",
    "#     except:\n",
    "#         print(f\"Error en {real}\")\n",
    "\n",
    "# output_file = get_fv_path() + os.path.sep + 'genuine_fv.csv'\n",
    "\n",
    "# with open(output_file, mode='w') as f:\n",
    "\n",
    "#     writer = csv.writer(f, delimiter=',', quotechar='\"',\n",
    "#                         quoting=csv.QUOTE_MINIMAL)\n",
    "#     writer.writerow([f\"f{i}\" for i in range(1,20)] + ['tag'])\n",
    "\n",
    "#     for url in fvs_real:\n",
    "#         writer.writerow(url)\n",
    "\n",
    "# f.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PHISHING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phishings = get_phish_tank_urls_csv(10000000).union(get_open_fish_urls())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fvs_phishing = []\n",
    "\n",
    "# for phishing in phishings:\n",
    "#     try:\n",
    "#         ph_entity = PHISH_FVG(phishing, tfidf)\n",
    "#         ph_entity.set_feature_vector()\n",
    "#         fvs_phishing.append(np.append(ph_entity.fv, [1]))\n",
    "\n",
    "#         if len(fvs_phishing) == 1100:\n",
    "#             break\n",
    "\n",
    "#     except:  # (ConnectionError, requests.exceptions.SSLError, requests.exceptions.ConnectTimeOut):\n",
    "#         print(phishing)\n",
    "\n",
    "# output_file = get_fv_path() + os.path.sep + \"phishing_fv.csv\"\n",
    "\n",
    "# with open(output_file, mode=\"w\") as f:\n",
    "#     writer = csv.writer(f, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#     writer.writerow([f\"f{i}\" for i in range(1, 20)] + [\"tag\"])\n",
    "\n",
    "#     for url in fvs_phishing:\n",
    "#         writer.writerow(url)\n",
    "\n",
    "# f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3aa2267a3633e362ab4cdc36738c6d0a45a450435ef5c859c0f11f93c27ebe6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
