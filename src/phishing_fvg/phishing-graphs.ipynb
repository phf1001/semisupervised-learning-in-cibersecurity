{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*-coding:utf-8 -*-\n",
    "'''\n",
    "@File    :   phishing-graphs.ipynb\n",
    "@Time    :   2023/03/30 21:02:43\n",
    "@Author  :   Patricia Hernando Fernández \n",
    "@Version :   1.0\n",
    "@Contact :   phf1001@alu.ubu.es\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PHISHING EXPERIMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    auc,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "import seaborn as sns\n",
    "\n",
    "# Changing paths to src\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(src_path)\n",
    "from models.classifiers.DemocraticCoClassifier import DemocraticCo\n",
    "from models.classifiers.TriTrainingClassifier import TriTraining\n",
    "from models.classifiers.CoForestClassifier import CoForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clss(rd=5):\n",
    "    # Random Forest\n",
    "    random_forest_all = RandomForestClassifier(\n",
    "        6, max_features=\"log2\", random_state=rd\n",
    "    )\n",
    "\n",
    "    # Co Forest\n",
    "    co_forest_six = CoForest(6, 0.75, max_features=\"log2\", random_state=rd)\n",
    "    co_forest_twenty = CoForest(20, 0.75, max_features=\"log2\", random_state=rd)\n",
    "\n",
    "    # Tri Training\n",
    "    tri_training = TriTraining(\n",
    "        DecisionTreeClassifier(random_state=rd),\n",
    "        GaussianNB(),\n",
    "        KNeighborsClassifier(),\n",
    "        random_state=rd,\n",
    "    )\n",
    "\n",
    "    # Democratic Co\n",
    "    democratic_co = DemocraticCo(\n",
    "        [\n",
    "            DecisionTreeClassifier(random_state=rd),\n",
    "            GaussianNB(),\n",
    "            KNeighborsClassifier(),\n",
    "        ],\n",
    "        random_state=rd,\n",
    "    )\n",
    "\n",
    "    cls = [\n",
    "        random_forest_all,\n",
    "        democratic_co,\n",
    "        co_forest_six,\n",
    "        co_forest_twenty,\n",
    "        tri_training,\n",
    "    ]\n",
    "\n",
    "    cls_names = [\"RF\", \"DC\", \"CoF_CLT\", \"CoF_PL\", \"TT\"]\n",
    "\n",
    "    return cls, cls_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_last_feature(features):\n",
    "    if features == \"F1-F8\":\n",
    "        return 0, 8\n",
    "\n",
    "    if features == \"F9\":\n",
    "        return 8, 9\n",
    "\n",
    "    if features == \"F10-F15\":\n",
    "        return 9, 15\n",
    "\n",
    "    if features == \"F16\":\n",
    "        return 15, 16\n",
    "\n",
    "    if features == \"F17-F19\":\n",
    "        return 16, 19\n",
    "\n",
    "    return 0, 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folds(features=\"all\", rd=5, file_name=\"./fv/results-5_fvg3/mix.csv\"):\n",
    "    df = pd.read_csv(filepath_or_buffer=file_name)\n",
    "    first_feature, last_feature = get_first_last_feature(features)\n",
    "    X = df[df.columns[first_feature:last_feature]].values\n",
    "    y = df.tag.values.astype(int)\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=rd)\n",
    "\n",
    "    return X, y, skf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comparation_graph(cls, cls_names, X, y, skf, rd=5, y_low_lim=0, curve='ROC', used_features='F1-F19'):\n",
    "\n",
    "    fig, axes = plt.subplot_mosaic(\n",
    "        \"AAABBBCCC;AAADDDEEE\", figsize=(10, 5), tight_layout=True)\n",
    "    colours = [\"#82e0aa\", \"#3eccf6\", \"#de88f3\", \"#f39c12\", \"#ff6961\"]\n",
    "    scores = []\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    f1s = []\n",
    "    AUCs = []\n",
    "    y_true_total = []\n",
    "    y_preds_total = {cls_name: [] for cls_name in cls_names}\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        L_train, U_train, Ly_train, Uy_train = train_test_split(\n",
    "            X_train, y_train, test_size=0.8, random_state=rd, stratify=y_train\n",
    "        )\n",
    "\n",
    "        scores_experiment = []\n",
    "        recalls_experiment = []\n",
    "        precisions_experiment = []\n",
    "        AUCs_experiment = []\n",
    "        f1s_experiment = []\n",
    "\n",
    "        for cl, cl_name in zip(cls, cls_names):\n",
    "            if cl_name == \"RF\":\n",
    "                #cl.fit(X_train, y_train)\n",
    "                cl.fit(L_train, Ly_train)\n",
    "            elif cl_name == \"CoF_CLT\":\n",
    "                cl.fit(\n",
    "                    L_train,\n",
    "                    Ly_train,\n",
    "                    U_train,\n",
    "                    w_init_criteria=\"confidence_L_all\",\n",
    "                )\n",
    "            elif cl_name == \"CoF_PL\":\n",
    "                cl.fit(\n",
    "                    L_train, Ly_train, U_train, w_init_criteria=\"percentage_L\"\n",
    "                )\n",
    "            else:\n",
    "                cl.fit(L_train, Ly_train, U_train)\n",
    "\n",
    "            y_pred = cl.predict(X_test)\n",
    "            scores_experiment.append(accuracy_score(y_test, y_pred))\n",
    "            recalls_experiment.append(recall_score(y_test, y_pred))\n",
    "            precisions_experiment.append(precision_score(y_test, y_pred))\n",
    "            f1s_experiment.append(f1_score(y_test, y_pred))\n",
    "            y_preds_total[cl_name] += list(y_pred)\n",
    "\n",
    "            if curve == \"ROC\":\n",
    "                fpr, tpr, _ = roc_curve(y_test, cl.predict_proba(X_test)[:, 1])\n",
    "                AUCs_experiment.append(auc(fpr, tpr))\n",
    "\n",
    "            else:\n",
    "                precision, recall, _ = precision_recall_curve(\n",
    "                    y_test, cl.predict_proba(X_test)[:, 1]\n",
    "                )\n",
    "                AUCs_experiment.append(auc(recall, precision))\n",
    "\n",
    "        scores.append(scores_experiment)\n",
    "        recalls.append(recalls_experiment)\n",
    "        precisions.append(precisions_experiment)\n",
    "        AUCs.append(AUCs_experiment)\n",
    "        f1s.append(f1s_experiment)\n",
    "        y_true_total += list(y_test)\n",
    "\n",
    "    axes['A'].set_ylabel(\"Accuracy\")\n",
    "    axes['A'].set_ylim(y_low_lim, 1.0)\n",
    "    axes['A'].bar(cls_names, np.mean(scores, axis=0), color=colours)\n",
    "\n",
    "    axes['B'].set_ylabel(\"Recall\")\n",
    "    axes['B'].set_ylim(y_low_lim, 1.0)\n",
    "    axes['B'].bar(cls_names, np.mean(recalls, axis=0), color=colours)\n",
    "\n",
    "    axes['C'].set_ylabel(\"Precision\")\n",
    "    axes['C'].set_ylim(y_low_lim, 1.0)\n",
    "    axes['C'].bar(cls_names, np.mean(precisions, axis=0), color=colours)\n",
    "\n",
    "    axes['D'].set_ylabel(\"F1\")\n",
    "    axes['D'].set_ylim(y_low_lim, 1.0)\n",
    "    axes['D'].bar(cls_names, np.mean(f1s, axis=0), color=colours)\n",
    "\n",
    "    axes['E'].set_ylabel(\"AUC\")\n",
    "    axes['E'].set_ylim(y_low_lim, 1.0)\n",
    "    axes['E'].bar(cls_names, np.mean(AUCs, axis=0), color=colours)\n",
    "\n",
    "    for i in ['A', 'B', 'C', 'D', 'E']:\n",
    "        axes[i].set_xticklabels(cls_names, rotation=45, ha=\"right\")\n",
    "\n",
    "    print(\"Resultados {}\".format(used_features))\n",
    "    plt.plot()\n",
    "\n",
    "    return y_true_total, y_preds_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, cls_name):\n",
    "\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    sorted_labels = [\"Legítimo\", \"Phishing\"]\n",
    "    plt.figure(figsize=(6.4,3))\n",
    "    sns.heatmap(matrix, annot=True, cmap=\"Blues\", fmt=\"g\", xticklabels=sorted_labels, yticklabels=sorted_labels)\n",
    "    plt.xlabel('Predicho')\n",
    "    plt.ylabel('Real')\n",
    "    plt.title('Matriz de confusión clasificador {}'.format(cls_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_important_atributes(X, y, max_depth=None):\n",
    "\n",
    "    if max_depth:\n",
    "        cls = DecisionTreeClassifier(max_depth=max_depth)\n",
    "\n",
    "    else:\n",
    "        cls = DecisionTreeClassifier()\n",
    "\n",
    "    cls.fit(X, y)\n",
    "    plt.figure(figsize=(20,20))\n",
    "    tree.plot_tree(cls)\n",
    "    plt.savefig('tree_high_dpi', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rf_feature_importance(features=\"all\", rd=5, file_name=\"./fv/results-5_fvg3/mix.csv\"):\n",
    "    df = pd.read_csv(filepath_or_buffer=file_name)\n",
    "\n",
    "    first_feature, last_feature = get_first_last_feature(features)\n",
    "    atributes = df.columns[first_feature:last_feature]\n",
    "    target = df.columns[-1]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=5000, n_jobs=-1, random_state=rd)\n",
    "    rf.fit(df[atributes], df[target])\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for atribute in atributes:\n",
    "        if len(atribute) == 3:\n",
    "            features.append(atribute)\n",
    "        else:\n",
    "            features.append(atribute[0] + '0' + atribute[1])\n",
    "\n",
    "    importances = rf.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "\n",
    "    plt.barh(range(len(indices)), importances[indices], color='#a8aff3', align='center')\n",
    "    plt.yticks(range(len(indices)), [features[i].upper() for i in indices])\n",
    "    plt.xlabel('Importancia relativa')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LINES GRAPH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(cls, cls_names, X, y, skf, rd=5, curve='ROC', percentage_labels=0.8):\n",
    "\n",
    "    scores = []\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    f1s = []\n",
    "    AUCs = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        L_train, U_train, Ly_train, Uy_train = train_test_split(\n",
    "            X_train, y_train, train_size=percentage_labels, random_state=rd, stratify=y_train\n",
    "        )\n",
    "\n",
    "        scores_experiment = []\n",
    "        recalls_experiment = []\n",
    "        precisions_experiment = []\n",
    "        AUCs_experiment = []\n",
    "        f1s_experiment = []\n",
    "\n",
    "        for cl, cl_name in zip(cls, cls_names):\n",
    "            if cl_name == \"RF\":\n",
    "                cl.fit(L_train, Ly_train)\n",
    "\n",
    "            elif cl_name == \"CoF_CLT\":\n",
    "                cl.fit(\n",
    "                    L_train,\n",
    "                    Ly_train,\n",
    "                    U_train,\n",
    "                    w_init_criteria=\"confidence_L_all\",\n",
    "                )\n",
    "            elif cl_name == \"CoF_PL\":\n",
    "                cl.fit(\n",
    "                    L_train, Ly_train, U_train, w_init_criteria=\"percentage_L\"\n",
    "                )\n",
    "            else:\n",
    "                cl.fit(L_train, Ly_train, U_train)\n",
    "\n",
    "            y_pred = cl.predict(X_test)\n",
    "            scores_experiment.append(accuracy_score(y_test, y_pred))\n",
    "            recalls_experiment.append(recall_score(y_test, y_pred))\n",
    "            precisions_experiment.append(precision_score(y_test, y_pred))\n",
    "            f1s_experiment.append(f1_score(y_test, y_pred))\n",
    "\n",
    "            if curve == \"ROC\":\n",
    "                fpr, tpr, _ = roc_curve(y_test, cl.predict_proba(X_test)[:, 1])\n",
    "                AUCs_experiment.append(auc(fpr, tpr))\n",
    "\n",
    "            else:\n",
    "                precision, recall, _ = precision_recall_curve(\n",
    "                    y_test, cl.predict_proba(X_test)[:, 1]\n",
    "                )\n",
    "                AUCs_experiment.append(auc(recall, precision))\n",
    "\n",
    "        scores.append(scores_experiment)\n",
    "        recalls.append(recalls_experiment)\n",
    "        precisions.append(precisions_experiment)\n",
    "        AUCs.append(AUCs_experiment)\n",
    "        f1s.append(f1s_experiment)\n",
    "\n",
    "\n",
    "    accuracys = np.mean(scores, axis=0)\n",
    "    recalls = np.mean(recalls, axis=0)\n",
    "    precisions = np.mean(precisions, axis=0)\n",
    "    AUCs = np.mean(AUCs, axis=0)\n",
    "    f1s = np.mean(f1s, axis=0)\n",
    "\n",
    "    return accuracys, recalls, precisions, f1s, AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_scores(percentages, metrics):\n",
    "    _, cls_names = get_clss()\n",
    "    scores_clss = {cls: {metric: [] for metric in metrics} for cls in cls_names}\n",
    "\n",
    "    for i in percentages:\n",
    "        clss, cls_names = get_clss()\n",
    "        X, y, skf = get_folds(\n",
    "            \"all\", file_name=\"./fv/results-5_fvg3/mix.csv\"\n",
    "        )\n",
    "\n",
    "        accuracys, recalls, precisions, f1s, AUCs = get_scores(\n",
    "            clss, cls_names, X, y, skf, curve=\"ROC\", percentage_labels=i\n",
    "        )\n",
    "\n",
    "        for i, cls_name in enumerate(cls_names):\n",
    "            scores_clss[cls_name][\"Accuracy\"].append(accuracys[i])\n",
    "            scores_clss[cls_name][\"Recall\"].append(recalls[i])\n",
    "            scores_clss[cls_name][\"Precision\"].append(precisions[i])\n",
    "            scores_clss[cls_name][\"F1\"].append(f1s[i])\n",
    "            scores_clss[cls_name][\"AUC\"].append(AUCs[i])\n",
    "\n",
    "    with open(\"results_classifiers.txt\", \"w\") as f:\n",
    "        f.write(json.dumps(scores_clss, indent=4))\n",
    "    f.close()\n",
    "\n",
    "    return scores_clss\n",
    "\n",
    "\n",
    "def get_evolution_graph(y_low_lim=0.6):\n",
    "    _, cls_names = get_clss()\n",
    "    percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    percentage_labels = [\"10%\", \"20%\", \"30%\", \"40%\", \"50%\", \"60%\", \"70%\", \"80%\", \"90%\"]\n",
    "    metrics = [\"Accuracy\", \"Recall\", \"Precision\", \"F1\", \"AUC\"]\n",
    "    scores_clss = run_all_scores(percentages, metrics)\n",
    "\n",
    "    fig, axes = plt.subplot_mosaic(\n",
    "        \"AA;BB;CC;DD\", figsize=(10, 20), tight_layout=True\n",
    "    )\n",
    "\n",
    "    colours = [\"#82e0aa\", \"#3eccf6\", \"#de88f3\", \"#f39c12\", \"#ff6961\"]\n",
    "    markers = [\"o\", \"v\", \"s\", \"P\", \"*\"]\n",
    "\n",
    "    for metric, ax in zip(metrics, [\"A\", \"B\", \"C\", \"D\"]):\n",
    "\n",
    "        fig_2, axes_2 = plt.subplot_mosaic(\n",
    "        \"AAAAAAA;AAAAAAA\", figsize=(10, 6), tight_layout=True)\n",
    "\n",
    "        for i, cls in enumerate(scores_clss):\n",
    "            for obj in [axes_2['A'], axes[ax]]:\n",
    "                obj.plot(\n",
    "                    percentage_labels,\n",
    "                    scores_clss[cls][metric],\n",
    "                    color=colours[i],\n",
    "                    linestyle=\"-\",\n",
    "                    marker=markers[i],\n",
    "                    linewidth=0.8,\n",
    "                    label=cls,\n",
    "                )\n",
    "\n",
    "                obj.set_ylabel(metric)\n",
    "                obj.set_xlabel(\"Labels (%)\")\n",
    "                obj.set_ylim(y_low_lim, 1.0)\n",
    "\n",
    "    fig.legend(\n",
    "        cls_names,\n",
    "        bbox_to_anchor=(0.5, 1.02),\n",
    "        loc=\"upper center\",\n",
    "        ncol=len(cls_names),\n",
    "    )\n",
    "\n",
    "    fig_2.legend(\n",
    "        cls_names,\n",
    "        bbox_to_anchor=(0.5, 1.05),\n",
    "        loc=\"upper center\",\n",
    "        ncol=len(cls_names),\n",
    "    )\n",
    "\n",
    "    plt.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EXPERIMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_evolution_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(used_features='all'):\n",
    "    clss, cls_names = get_clss()\n",
    "    X, y, skf = get_folds(used_features)\n",
    "    y_true_total, y_preds_total = get_comparation_graph(clss, cls_names, X, y, skf, curve='ROC', used_features=used_features)\n",
    "\n",
    "    for cls_name in cls_names:\n",
    "        plot_confusion_matrix(y_true_total, y_preds_total[cls_name], cls_name=cls_name)\n",
    "\n",
    "    plot_important_atributes(X, y)\n",
    "    plot_important_atributes(X, y, max_depth=3)\n",
    "    plot_rf_feature_importance(used_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = 'all'\n",
    "run_all(used_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = 'F1-F8'\n",
    "run_all(used_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = 'F9'\n",
    "run_all(used_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = 'F10-F15'\n",
    "run_all(used_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = 'F16'\n",
    "run_all(used_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = 'F17-F19'\n",
    "run_all(used_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3aa2267a3633e362ab4cdc36738c6d0a45a450435ef5c859c0f11f93c27ebe6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
