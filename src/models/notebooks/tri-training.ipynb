{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRI-TRAINING - ☘** \n",
    "\n",
    "##### **Autora: Patricia Hernando Fernández**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_iris()\n",
    "\n",
    "X = np.array(dataset.data)\n",
    "y = np.array(dataset.target)\n",
    "\n",
    "rd = np.random.RandomState(5)\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=rd)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        L_train, U_train, Ly_train, Uy_train = train_test_split(X_train, y_train, test_size=0.8, random_state=rd, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tri_Training:  \n",
    "\n",
    "    def __init__(self, L, y, U, h_0, h_1, h_2, random_state=None):\n",
    "        \n",
    "        self.n = 3\n",
    "\n",
    "        self.n_L = L.shape[0]\n",
    "        self.L = L\n",
    "        self.y = y\n",
    "        self.U = U\n",
    "\n",
    "        self.mask_L = np.zeros(shape=((self.L.shape[0]), self.n), dtype=int, order='C')\n",
    "\n",
    "        self.classes = np.unique(y)\n",
    "        self.rd = self.check_random_state(random_state)\n",
    "\n",
    "        self.classifiers = self.initialize_classifiers([h_0, h_1, h_2])\n",
    "\n",
    "\n",
    "    def initialize_classifiers(self, cls):\n",
    "\n",
    "        classifiers = {}\n",
    "\n",
    "        # La máscara en este va a sobrar, no hace falta memoria se reentrena con todo L\n",
    "        for i in range(self.n):\n",
    "\n",
    "            # Además, igual es mejor bootstrapear al 100% del tamaño de L (replace)\n",
    "            rand_rows = self.rd.choice(a = np.arange(0, self.n_L), replace = True, size=(int(0.8*self.n_L)) )\n",
    "            self.mask_L[rand_rows, i] = 1\n",
    "            classifiers[i] = cls[i].fit(self.L[rand_rows, :], self.y[rand_rows])\n",
    "\n",
    "        return classifiers\n",
    "\n",
    "\n",
    "    def measure_error(self, i):\n",
    "        \"\"\"\n",
    "            In detail, the classification error of the\n",
    "            hypothesis is approximated through dividing the number\n",
    "            of labeled examples on which both hj and hk make\n",
    "            incorrect classification by the number of labeled examples\n",
    "            on which the classification made by hj is the same as that\n",
    "            made by hk.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction_j = self.classifiers[(i+1) % 3].predict(self.L)\n",
    "        prediction_k = self.classifiers[(i+2) % 3].predict(self.L)\n",
    "\n",
    "        incorrect_classification = np.logical_and(prediction_j != self.y, prediction_k == prediction_j)\n",
    "        concordance = (prediction_j == prediction_k)\n",
    "\n",
    "        return sum(incorrect_classification) / sum(concordance)\n",
    "\n",
    "\n",
    "    def create_pseudolabeled_set(self, i):\n",
    "        \"\"\"\n",
    "        when two models agree on the label, save it\n",
    "        \"\"\"\n",
    "\n",
    "        U_y_j = self.classifiers[(i+1) % 3].predict(self.U)\n",
    "        U_y_k = self.classifiers[(i+2) % 3].predict(self.U)\n",
    "\n",
    "        concordances = (U_y_j == U_y_k)\n",
    "\n",
    "        return (self.U[concordances], U_y_j[concordances])\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "\n",
    "        previous_e = [0.5 for i in range(self.n)]\n",
    "        previous_l = [0 for i in range(self.n)]\n",
    "\n",
    "        e = [0 for i in range(self.n)]\n",
    "        l = [0 for i in range(self.n)]\n",
    "\n",
    "        new_data = True\n",
    "\n",
    "        while new_data:\n",
    "\n",
    "            cls_changes = np.array([False for i in range(self.n)])\n",
    "            cls_pseudo_updates = [(np.array([]), np.array([])) for i in range(self.n)]\n",
    "\n",
    "            for i in range(self.n):\n",
    "\n",
    "                e[i] = self.measure_error(i)\n",
    "\n",
    "                if e[i] < previous_e[i]:\n",
    "                    cls_pseudo_updates[i] = self.create_pseudolabeled_set(i)\n",
    "\n",
    "                if previous_l[i] == 0:\n",
    "                    previous_l[i] = ((e[i] / (previous_e[i]-e[i])) + 1)\n",
    "\n",
    "                L_i_size = cls_pseudo_updates[i][0].shape[0]\n",
    "\n",
    "                if previous_l[i] < L_i_size:\n",
    "\n",
    "                    if e[i] * L_i_size < previous_e[i] * previous_l[i]:\n",
    "                        cls_changes[i] = True\n",
    "                    \n",
    "                    elif previous_l[i] > (e[i] / (previous_e[i] - e[i])):\n",
    "\n",
    "                        L_index = self.rd.choice(L_i_size, int( (previous_e[i] * previous_l[i] / e[i]) - 1))\n",
    "                        cls_pseudo_updates[i] = (cls_pseudo_updates[i][0][L_index], cls_pseudo_updates[i][1][L_index])\n",
    "                        cls_changes[i] = True\n",
    "\n",
    "            if cls_changes.sum() == 0:\n",
    "                new_data = False\n",
    "\n",
    "            else:\n",
    "\n",
    "                for i in np.fromiter(self.classifiers.keys(), dtype=int)[cls_changes]:\n",
    "\n",
    "                    X_train = np.concatenate((self.L, cls_pseudo_updates[i][0]))\n",
    "                    y_train = np.concatenate((self.y, cls_pseudo_updates[i][1]))\n",
    "                    self.classifiers[i] = self.classifiers[i].fit(X_train, y_train)\n",
    "\n",
    "                    previous_e[i] = e[i]\n",
    "                    previous_l[i] = cls_pseudo_updates[i][0].shape[0] #Tamaño de Li anterior\n",
    "\n",
    "\n",
    "\n",
    "    def check_random_state(self, seed):\n",
    "        \"\"\"\n",
    "        Turn seed into a np.random.RandomState instance.\n",
    "        Source: SkLearn\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : None, int or instance of RandomState\n",
    "            If seed is None, return the RandomState singleton used by np.random.\n",
    "            If seed is an int, return a new RandomState instance seeded with seed.\n",
    "            If seed is already a RandomState instance, return it.\n",
    "            Otherwise raise ValueError.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.random.RandomState\n",
    "            The random state object based on seed parameter.\n",
    "        \"\"\"\n",
    "\n",
    "        if seed is None or seed is np.random:\n",
    "            return np.random.mtrand._rand\n",
    "\n",
    "        if isinstance(seed, numbers.Integral):\n",
    "            return np.random.RandomState(seed)\n",
    "\n",
    "        if isinstance(seed, np.random.RandomState):\n",
    "            return seed\n",
    "            \n",
    "        raise ValueError(\"%r cannot be used to seed a numpy.random.RandomState instance\" % seed)\n",
    "\n",
    "\n",
    "    # def single_predict(self, sample: np.array): \n",
    "    #     \"\"\"\n",
    "    #     Returns the class predicted by tri-training.\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     sample: np_array\n",
    "    #         sample to predict\n",
    "\n",
    "    #     Returns\n",
    "    #     -------\n",
    "    #     np.array:\n",
    "    #         label predicted by tri-training.\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     count = {i: 0  for i in self.classes}\n",
    "\n",
    "    #     for i in (cls.predict([sample])[0] for cls in self.classifiers.values()):\n",
    "    #         count[i]+= 1\n",
    "\n",
    "    #     max_agreement = max(count.values())\n",
    "    #     return list(count.keys())[list(count.values()).index(max_agreement)]\n",
    "\n",
    "\n",
    "    # def predict(self, samples: np.array) -> np.array:\n",
    "    #     \"\"\"\n",
    "    #     Returns the labels predicted by the coforest\n",
    "    #     for a given data.\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     samples: np_array\n",
    "    #         samples to predict\n",
    "\n",
    "    #     Returns\n",
    "    #     -------\n",
    "    #     np.array:\n",
    "    #         labels predicted by the coforest.\n",
    "    #     \"\"\"\n",
    "        \n",
    "    #     samples = (lambda x: np.expand_dims(x, axis=0) if x.ndim == 1 else x)(samples)\n",
    "    #     return np.array([self.single_predict(sample) for sample in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h_1 = DecisionTreeClassifier()\n",
    "h_2 = DecisionTreeClassifier()\n",
    "h_3 = DecisionTreeClassifier()\n",
    "\n",
    "t_t = Tri_Training(L_train, Ly_train, U_train, h_1, h_2, h_3)\n",
    "t_t.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5061e2ebcf242305dcdb45d871ef5cd4ba433365f314bc2418d41c77ce076e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
