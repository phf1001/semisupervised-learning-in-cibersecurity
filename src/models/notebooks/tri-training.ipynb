{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRI-TRAINING - ☘** \n",
    "\n",
    "##### **Autora: Patricia Hernando Fernández**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "import numpy as np\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tri_Training:  \n",
    "\n",
    "    def __init__(self, h_0, h_1, h_2, random_state=None):\n",
    "        \"\"\"\n",
    "        Constructor. Creates the tri-training instance.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        h_0, h_1, h_2:\n",
    "            Classifiers\n",
    "        random_state:\n",
    "            Random object or seed\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = 3\n",
    "        self.classes = []\n",
    "        self.rd = self.check_random_state(random_state)\n",
    "        self.classifiers = {0 : h_0, 1: h_1, 2: h_2}\n",
    "\n",
    "\n",
    "    def fit(self, L, y, U):\n",
    "        \"\"\"\n",
    "        Trains the tri-training ensemble using Zhi-Hua Zhou\n",
    "        Algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        L: np.array\n",
    "            Labeled data used for training\n",
    "        y: np.array\n",
    "            Labeled data tags used for training\n",
    "        U: np.array\n",
    "            Unlabeled data used for training\n",
    "        \"\"\"\n",
    "\n",
    "        self.initialize_classifiers(L, y)\n",
    "        self.classes = np.unique(y)\n",
    "\n",
    "        previous_e = [0.5 for i in range(self.n)]\n",
    "        previous_l = [0.0 for i in range(self.n)]\n",
    "\n",
    "        e = [0.0 for i in range(self.n)]\n",
    "        l = [0.0 for i in range(self.n)]\n",
    "\n",
    "        new_data = True\n",
    "\n",
    "        while new_data:\n",
    "\n",
    "            cls_changes = np.array([False for i in range(self.n)])\n",
    "            cls_pseudo_updates = [() for i in range(self.n)]\n",
    "\n",
    "            for i in range(self.n):\n",
    "\n",
    "                e[i] = self.measure_error(i, L, y)\n",
    "\n",
    "                if e[i] < previous_e[i]:\n",
    "                    cls_pseudo_updates[i] = self.create_pseudolabeled_set(i, U)\n",
    "\n",
    "                    if previous_l[i] == 0:\n",
    "                        previous_l[i] = ((e[i] / (previous_e[i]-e[i])) + 1)\n",
    "\n",
    "                    L_i_size = cls_pseudo_updates[i][0].shape[0]\n",
    "\n",
    "                    if previous_l[i] < L_i_size:\n",
    "\n",
    "                        if e[i] * L_i_size < previous_e[i] * previous_l[i]:\n",
    "                            cls_changes[i] = True\n",
    "                        \n",
    "                        elif previous_l[i] > (e[i] / (previous_e[i] - e[i])):\n",
    "\n",
    "                            L_index = self.rd.choice(L_i_size, int((previous_e[i] * previous_l[i] / e[i]) - 1))\n",
    "                            cls_pseudo_updates[i] = (cls_pseudo_updates[i][0][L_index], cls_pseudo_updates[i][1][L_index])\n",
    "                            cls_changes[i] = True\n",
    "\n",
    "            if cls_changes.sum() == 0:\n",
    "                new_data = False\n",
    "\n",
    "            else:\n",
    "\n",
    "                for i in np.fromiter(self.classifiers.keys(), dtype=int)[cls_changes]:\n",
    "\n",
    "                    X_train = np.concatenate((L, cls_pseudo_updates[i][0]))\n",
    "                    y_train = np.concatenate((y, cls_pseudo_updates[i][1]))\n",
    "                    self.classifiers[i] = self.classifiers[i].fit(X_train, y_train)\n",
    "\n",
    "                    previous_e[i] = e[i]\n",
    "                    previous_l[i] = cls_pseudo_updates[i][0].shape[0] #Tamaño de Li anterior\n",
    "\n",
    "\n",
    "    def initialize_classifiers(self, L, y, percentage=0.8):\n",
    "        \"\"\"\n",
    "        Initializes each base classifier bootstrapping\n",
    "        from L.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        L: np.array\n",
    "            Labeled data used for training\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(self.n):\n",
    "            rand_rows = self.rd.choice(L.shape[0], replace = True, size = (int(percentage * L.shape[0])) )\n",
    "            self.classifiers[i] = self.classifiers[i].fit(L[rand_rows, :], y[rand_rows])\n",
    "\n",
    "\n",
    "    def measure_error(self, i, L, y):\n",
    "        \"\"\"\n",
    "        The classification error is approximated through \n",
    "        dividing the number of labeled examples on which \n",
    "        both hj and hk make incorrect classification by \n",
    "        the number of labeled examples on which the \n",
    "        classification made by hj is the same as that made \n",
    "        by hk.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i: int\n",
    "            Excluded classifier index\n",
    "        L: np.array\n",
    "            Labeled data used for training\n",
    "        y: np.array\n",
    "            Labeled data tags used for training\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction_j = self.classifiers[(i+1) % self.n].predict(L)\n",
    "        prediction_k = self.classifiers[(i+2) % self.n].predict(L)\n",
    "\n",
    "        incorrect_classification = np.logical_and(prediction_j != y, prediction_k == prediction_j)\n",
    "        concordance = (prediction_j == prediction_k)\n",
    "\n",
    "        return sum(incorrect_classification) / sum(concordance)\n",
    "\n",
    "\n",
    "    def create_pseudolabeled_set(self, i, U):\n",
    "        \"\"\"\n",
    "        Li is created by saving those samples in which\n",
    "        the other two classifiers agree on the tag.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i: int\n",
    "            Excluded classifier index\n",
    "        U: np.array\n",
    "            Unlabeled data used for training\n",
    "        \"\"\"\n",
    "\n",
    "        U_y_j = self.classifiers[(i+1) % self.n].predict(U)\n",
    "        U_y_k = self.classifiers[(i+2) % self.n].predict(U)\n",
    "\n",
    "        concordances = (U_y_j == U_y_k)\n",
    "\n",
    "        return (U[concordances], U_y_k[concordances])\n",
    "\n",
    "\n",
    "    def check_random_state(self, seed=None):\n",
    "        \"\"\"\n",
    "        Turn seed into a np.random.RandomState instance.\n",
    "        Source: SkLearn\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : None, int or instance of RandomState\n",
    "            If None, return the RandomState singleton.\n",
    "            If int, return a new RandomState seeded with seed.\n",
    "            If RandomState instance, return it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.random.RandomState\n",
    "            The random state object based on seed parameter.\n",
    "        \"\"\"\n",
    "\n",
    "        if seed is None or seed is np.random:\n",
    "            return np.random.mtrand._rand\n",
    "\n",
    "        if isinstance(seed, numbers.Integral):\n",
    "            return np.random.RandomState(seed)\n",
    "\n",
    "        if isinstance(seed, np.random.RandomState):\n",
    "            return seed\n",
    "\n",
    "\n",
    "    def single_predict(self, sample): \n",
    "        \"\"\"\n",
    "        Returns the class predicted by tri-training.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample: np_array\n",
    "            sample to predict\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array:\n",
    "            label predicted by tri-training.\n",
    "        \"\"\"\n",
    "\n",
    "        count = {i: 0  for i in self.classes}\n",
    "\n",
    "        for i in (cls.predict([sample])[0] for cls in self.classifiers.values()):\n",
    "            count[i]+= 1\n",
    "\n",
    "        max_agreement = max(count.values())\n",
    "        return list(count.keys())[list(count.values()).index(max_agreement)]\n",
    "\n",
    "\n",
    "    def predict(self, samples):\n",
    "        \"\"\"\n",
    "        Returns the labels predicted by the tri-training\n",
    "        for a given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples: np_array\n",
    "            samples to predict\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array:\n",
    "            labels predicted by tri-training.\n",
    "        \"\"\"\n",
    "        \n",
    "        samples = (lambda x: np.expand_dims(x, axis=0) if x.ndim == 1 else x)(samples)\n",
    "        return np.array([self.single_predict(sample) for sample in samples])\n",
    "\n",
    "\n",
    "    def single_predict_proba(self, sample):\n",
    "        \"\"\"\n",
    "        Returns the probability for each class \n",
    "        predicted by tri-training for a given sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample: np_array\n",
    "            sample to predict\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array:\n",
    "            array containing probability for each class.\n",
    "        \"\"\"\n",
    "\n",
    "        count = {i: 0  for i in self.classes}\n",
    "\n",
    "        for i in (cls.predict([sample])[0] for cls in self.classifiers.values()):\n",
    "                count[i]+= 1\n",
    "\n",
    "        votes = np.array(list(count.values()))\n",
    "        return votes / self.n\n",
    "\n",
    "\n",
    "    def predict_proba(self, samples: np.array):\n",
    "        \"\"\"\n",
    "        Returns the probabilities predicted by \n",
    "        tri-training for a given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples: np_array\n",
    "            samples to predict\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array:\n",
    "            array containing one array for each\n",
    "            sample with probabilities for each \n",
    "            class.\n",
    "        \"\"\"\n",
    "\n",
    "        samples = (lambda x: np.expand_dims(x, axis=0) if x.ndim == 1 else x)(samples)\n",
    "        return np.array([self.single_predict_proba(sample) for sample in samples])\n",
    "\n",
    "\n",
    "    def score(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Calculates the number of hits by tri-training.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: np_array\n",
    "            Samples to predict\n",
    "        y: np_array\n",
    "            True tags\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            percentage of hits.\n",
    "        \"\"\"\n",
    "        y_predictions = self.predict(X)\n",
    "        return np.count_nonzero(y_predictions==y_true)/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2\n",
      " 2]\n",
      "0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "dataset = load_iris()\n",
    "\n",
    "X = np.array(dataset.data)\n",
    "y = np.array(dataset.target)\n",
    "\n",
    "rd = np.random.RandomState(5)\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=rd)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        L_train, U_train, Ly_train, Uy_train = train_test_split(X_train, y_train, test_size=0.8, random_state=rd, stratify=y_train)\n",
    "        \n",
    "h_1 = DecisionTreeClassifier()\n",
    "h_2 = DecisionTreeClassifier()\n",
    "h_3 = DecisionTreeClassifier()\n",
    "\n",
    "t_t = Tri_Training(h_1, h_2, h_3, 5)\n",
    "t_t.fit(L_train, Ly_train, U_train)\n",
    "\n",
    "print(t_t.predict(X_test))\n",
    "print(t_t.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5061e2ebcf242305dcdb45d871ef5cd4ba433365f314bc2418d41c77ce076e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
