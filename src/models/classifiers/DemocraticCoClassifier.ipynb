{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numbers\n",
    "from copy import deepcopy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_digits, load_iris, load_wine, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemocraticCo:  \n",
    "\n",
    "    def __init__(self, base_cls, random_state=None):\n",
    "        \"\"\"\n",
    "        Constructor. Creates the co-training instance.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        base_cls:\n",
    "            Classifiers\n",
    "        random_state:\n",
    "            Random object or seed\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = len(base_cls)\n",
    "        self.classes = []\n",
    "        self.rd = self.check_random_state(random_state)\n",
    "        self.classifiers = {i: base_cls[i] for i in range(self.n)}\n",
    "        self.confidence_intervals = []\n",
    "\n",
    "\n",
    "    def fit(self, L, y, U):\n",
    "        \"\"\"\n",
    "        Trains the Democratic-Co.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        L: np.array\n",
    "            Labeled data used for training\n",
    "        y: np.array\n",
    "            Labeled data tags used for training\n",
    "        U: np.array\n",
    "            Unlabeled data used for training\n",
    "        \"\"\"\n",
    "\n",
    "        classes = np.unique(y)\n",
    "        self.classes = classes\n",
    "        changes = True\n",
    "\n",
    "        e = [0.0 for i in range(self.n)]\n",
    "        q = [0.0 for i in range(self.n)]\n",
    "        e_prime = [0.0 for i in range(self.n)]\n",
    "        q_prime = [0.0 for i in range(self.n)]\n",
    "        cls_updates = [(list(L), list(y)) for i in range(self.n)]\n",
    "        cls_proposed_updates = [([], []) for i in range(self.n)]\n",
    "\n",
    "\n",
    "        while changes:\n",
    "\n",
    "            cls_changes = np.array([False for i in range(self.n)])\n",
    "\n",
    "            hyps = deepcopy(self.classifiers)\n",
    "            for i in range(self.n):\n",
    "                X_train, y_train = cls_updates[i]\n",
    "                hyps[i] = hyps[i].fit(X_train, y_train)\n",
    "\n",
    "            matrix = [[set() for j in range(len(self.classes))] for x in range(len(U))]\n",
    "\n",
    "            for x in range(len(U)):\n",
    "                for i in range(self.n):\n",
    "                    j = hyps[i].predict([U[x]])[0]\n",
    "                    c = matrix[x][j]\n",
    "                    c.add(i)\n",
    "                    matrix[x][j] = c\n",
    "\n",
    "            y_U = []\n",
    "            for x in matrix:\n",
    "                x = [len(c) for c in x]\n",
    "                n_max = np.amax(x)\n",
    "                tag_index = x.index(n_max)\n",
    "                y_U.append(classes[tag_index])\n",
    "\n",
    "            # Choose which exs to propose for labeling\n",
    "            w = []\n",
    "            for i in range(self.n):\n",
    "                li, hi = self.confidence_interval_cesar(hyps[i], L, y)\n",
    "                w.append((li + hi) / 2)\n",
    "                \n",
    "            cls_proposed_updates = [([], []) for i in range(self.n)]\n",
    "\n",
    "            for x in range(len(U)):\n",
    "\n",
    "                voted_tag = y_U[x]\n",
    "                index = np.where(classes == voted_tag)[0][0]\n",
    "                cls_agree_tag = matrix[x][index]\n",
    "\n",
    "                a1 = 0  #sumatorio de los que coinciden por sus pesos\n",
    "                for cls in cls_agree_tag:\n",
    "                    a1 += w[cls]\n",
    "\n",
    "                a2 = 0\n",
    "                for tag in classes:\n",
    "                    if tag != voted_tag:\n",
    "                        index = np.where(classes == tag)[0][0]\n",
    "\n",
    "                        weight_tag = 0\n",
    "                        for cls in matrix[x][index]:\n",
    "                            weight_tag += w[cls]\n",
    "                        a2 = max(a2, weight_tag)\n",
    "\n",
    "                if a1 > a2:\n",
    "                    for cls in (set(self.classifiers.keys()) - cls_agree_tag):\n",
    "                        Li_prime, y_Li_prime = cls_proposed_updates[i]\n",
    "                        Li_prime.append(U[x])\n",
    "                        y_Li_prime.append(y_U[x])\n",
    "                        cls_proposed_updates[i] = (Li_prime, y_Li_prime)\n",
    "\n",
    "\n",
    "            # Estimate if adding this is better\n",
    "            for i in range(self.n):\n",
    "\n",
    "                l = []\n",
    "                for j in range(self.n):\n",
    "                    Lj, y_Lj = cls_updates[j]\n",
    "                    lj, hj = self.confidence_interval_cesar(hyps[j], Lj, y_Lj)\n",
    "                    l.append(lj)\n",
    "\n",
    "                Li, y_Li = cls_updates[i]\n",
    "                Li_prime, y_Li_prime = cls_proposed_updates[i]\n",
    "\n",
    "                if len(Li_prime) > 0:\n",
    "                    Li_union_Li_prime = Li + Li_prime\n",
    "                else:\n",
    "                    Li_union_Li_prime = Li\n",
    "\n",
    "                q[i] = len(Li) * (1 - 2 * (e[i] / len(Li))) ** 2\n",
    "                e_prime[i] = (1 - np.mean(l)) * len(Li_prime)\n",
    "                q_prime[i] = len(Li_union_Li_prime) * ( 1 - (2*(e[i] + e_prime[i]) / len(Li_union_Li_prime)) ) ** 2\n",
    "\n",
    "                if q_prime[i] > q[i]:\n",
    "                    cls_changes[i] = True\n",
    "                    cls_updates[i] = (Li_union_Li_prime, y_Li + y_Li_prime)\n",
    "                    e[i] = e[i] + e_prime[i]\n",
    "\n",
    "        \n",
    "            if cls_changes.sum() == 0:\n",
    "                changes = False\n",
    "                self.classifiers = deepcopy(hyps)\n",
    "                self.confidence_intervals = [self.confidence_interval_cesar(cls, L, y) for cls in hyps]\n",
    "                self.w = [((li + hi)/2) for li, hi in self.confidence_intervals]\n",
    "\n",
    "\n",
    "    def confidence_interval_cesar(self, cls, L, y):\n",
    "        \n",
    "        y_pred = cls.predict(L)\n",
    "        n_total = len(y)\n",
    "        n_hits = (y_pred == y).sum()\n",
    "        p_hat = n_hits / n_total\n",
    "        margin = 1.96 * math.sqrt( p_hat * (1 - p_hat) / n_total)\n",
    "\n",
    "        return (p_hat - margin, p_hat + margin)\n",
    "\n",
    "\n",
    "    def confidence_interval_alvar(self, cls, L, y):\n",
    "\n",
    "        y_pred = cls.predict(L)\n",
    "        n_total = len(y)\n",
    "        n_hits = (y_pred == y).sum()\n",
    "\n",
    "        zSq = 1.96 * 1.96\n",
    "        f = n_hits / n_total\n",
    "\n",
    "        left = f + (zSq / (2 * n_total))\n",
    "        div = 1 + (zSq / n_total)\n",
    "        sq = 1.96 * math.sqrt((f / n_total) - ((f * f) / n_total) + (zSq / (4 * n_total ** 2)))\n",
    "\n",
    "        return ((left - sq) / div, (left + sq) / div)\n",
    "\n",
    "\n",
    "    def check_random_state(self, seed=None):\n",
    "        \"\"\"\n",
    "        Turn seed into a np.random.RandomState instance.\n",
    "        Source: SkLearn\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : None, int or instance of RandomState\n",
    "            If None, return the RandomState singleton.\n",
    "            If int, return a new RandomState seeded with seed.\n",
    "            If RandomState instance, return it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.random.RandomState\n",
    "            The random state object based on seed parameter.\n",
    "        \"\"\"\n",
    "\n",
    "        if seed is None or seed is np.random:\n",
    "            return np.random.mtrand._rand\n",
    "\n",
    "        if isinstance(seed, numbers.Integral):\n",
    "            return np.random.RandomState(seed)\n",
    "\n",
    "        if isinstance(seed, np.random.RandomState):\n",
    "            return seed\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, samples):\n",
    "        \"\"\"\n",
    "        Returns the labels predicted by the democratic-co\n",
    "        for a given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples: np_array\n",
    "            samples to predict\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array:\n",
    "            labels predicted by democratic-co\n",
    "        \"\"\"\n",
    "        \n",
    "        samples = (lambda x: np.expand_dims(x, axis=0) if x.ndim == 1 else x)(samples)\n",
    "        return np.array([self.single_predict(sample) for sample in samples])\n",
    "\n",
    "\n",
    "\n",
    "    def single_predict(self, sample): \n",
    "        \"\"\"\n",
    "        Returns the class predicted by democratic-co\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample: np_array\n",
    "            sample to predict\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array:\n",
    "            label predicted by democratic-co\n",
    "        \"\"\"\n",
    "\n",
    "        groups = {i: set()  for i in self.classes}\n",
    "\n",
    "        for id_cls, cls in self.classifiers.items():\n",
    "\n",
    "            if self.w[id_cls] > 0.5:\n",
    "                prediction = cls.predict(sample)\n",
    "                group = groups[prediction]\n",
    "                group.add(id_cls)\n",
    "                groups[prediction] = group\n",
    "\n",
    "\n",
    "        max_confidence = -1\n",
    "        chosen_group = None\n",
    "\n",
    "        for j, group in groups:\n",
    "\n",
    "            n_cls = len(group)\n",
    "            group_weight = 0\n",
    "            for id_cls in group:\n",
    "                group_weight += self.w[id_cls]\n",
    "\n",
    "            average_confidence = ((n_cls + 0.5) / (n_cls + 1)) * (group_weight / n_cls)\n",
    "\n",
    "            if average_confidence > max_confidence:\n",
    "                max_confidence = average_confidence\n",
    "                chosen_group = j\n",
    "\n",
    "        return chosen_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0 = DecisionTreeClassifier()\n",
    "h_1 = GaussianNB()\n",
    "h_2 = KNeighborsClassifier()\n",
    "\n",
    "dataset = load_wine()\n",
    "\n",
    "X = np.array(dataset.data)\n",
    "y = np.array(dataset.target)\n",
    "\n",
    "rd = np.random.RandomState(5)\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=rd)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        L_train, U_train, Ly_train, Uy_train = train_test_split(X_train, y_train, test_size=0.8, random_state=rd, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m democratic_co \u001b[39m=\u001b[39m DemocraticCo([h_0, h_1, h_2], random_state\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m democratic_co\u001b[39m.\u001b[39;49mfit(L_train, Ly_train, U_train)\n",
      "Cell \u001b[1;32mIn [5], line 115\u001b[0m, in \u001b[0;36mDemocraticCo.fit\u001b[1;34m(self, L, y, U)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn):\n\u001b[0;32m    114\u001b[0m     Lj, y_Lj \u001b[39m=\u001b[39m cls_updates[j]\n\u001b[1;32m--> 115\u001b[0m     lj, hj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfidence_interval_cesar(hyps[j], Lj, y_Lj)\n\u001b[0;32m    116\u001b[0m     l\u001b[39m.\u001b[39mappend(lj)\n\u001b[0;32m    118\u001b[0m Li, y_Li \u001b[39m=\u001b[39m cls_updates[i]\n",
      "Cell \u001b[1;32mIn [5], line 145\u001b[0m, in \u001b[0;36mDemocraticCo.confidence_interval_cesar\u001b[1;34m(self, cls, L, y)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfidence_interval_cesar\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mcls\u001b[39m, L, y):\n\u001b[1;32m--> 145\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(L)\n\u001b[0;32m    146\u001b[0m     n_total \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(y)\n\u001b[0;32m    147\u001b[0m     n_hits \u001b[39m=\u001b[39m (y_pred \u001b[39m==\u001b[39m y)\u001b[39m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\patri\\source\\venvs\\tfg-01\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:226\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39m\"\"\"Predict the class labels for the provided data.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \n\u001b[0;32m    212\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39m    Class labels for each data sample.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muniform\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    224\u001b[0m     \u001b[39m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[0;32m    225\u001b[0m     \u001b[39m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m     neigh_ind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkneighbors(X, return_distance\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    227\u001b[0m     neigh_dist \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\patri\\source\\venvs\\tfg-01\\lib\\site-packages\\sklearn\\neighbors\\_base.py:814\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    808\u001b[0m     \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[0;32m    809\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    810\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m does not work with sparse matrices. Densify the data, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    811\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor set algorithm=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbrute\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    812\u001b[0m             \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_method\n\u001b[0;32m    813\u001b[0m         )\n\u001b[1;32m--> 814\u001b[0m     chunked_results \u001b[39m=\u001b[39m Parallel(n_jobs, prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m)(\n\u001b[0;32m    815\u001b[0m         delayed(_tree_query_parallel_helper)(\n\u001b[0;32m    816\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tree, X[s], n_neighbors, return_distance\n\u001b[0;32m    817\u001b[0m         )\n\u001b[0;32m    818\u001b[0m         \u001b[39mfor\u001b[39;49;00m s \u001b[39min\u001b[39;49;00m gen_even_slices(X\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], n_jobs)\n\u001b[0;32m    819\u001b[0m     )\n\u001b[0;32m    820\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    821\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39minternal: _fit_method not recognized\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\patri\\source\\venvs\\tfg-01\\lib\\site-packages\\joblib\\parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1040\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\patri\\source\\venvs\\tfg-01\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patri\\source\\venvs\\tfg-01\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\patri\\source\\venvs\\tfg-01\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\patri\\source\\venvs\\tfg-01\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\patri\\source\\venvs\\tfg-01\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\patri\\source\\venvs\\tfg-01\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\patri\\source\\venvs\\tfg-01\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\patri\\source\\venvs\\tfg-01\\lib\\site-packages\\sklearn\\neighbors\\_base.py:623\u001b[0m, in \u001b[0;36m_tree_query_parallel_helper\u001b[1;34m(tree, *args, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_tree_query_parallel_helper\u001b[39m(tree, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    618\u001b[0m     \u001b[39m\"\"\"Helper for the Parallel calls in KNeighborsMixin.kneighbors.\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \n\u001b[0;32m    620\u001b[0m \u001b[39m    The Cython method tree.query is not directly picklable by cloudpickle\u001b[39;00m\n\u001b[0;32m    621\u001b[0m \u001b[39m    under PyPy.\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 623\u001b[0m     \u001b[39mreturn\u001b[39;00m tree\u001b[39m.\u001b[39mquery(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "democratic_co = DemocraticCo([h_0, h_1, h_2], random_state=5)\n",
    "democratic_co.fit(L_train, Ly_train, U_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5061e2ebcf242305dcdb45d871ef5cd4ba433365f314bc2418d41c77ce076e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
