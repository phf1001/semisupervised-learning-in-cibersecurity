{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PHISHING VECTOR GENERATOR** ðŸŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\patri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from user_browsing import user_browsing\n",
    "from xml.etree import ElementTree as ET\n",
    "from urllib.parse import urlparse\n",
    "from os import path\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "from phishing_utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PHISH_FVG:\n",
    "\n",
    "    def __init__(self, url, fichero = 'html_dump'):\n",
    "\n",
    "        self.url = url\n",
    "        self.base, self.path = self.process_url(self.url)\n",
    "\n",
    "        self.fv = np.array([-1 for i in range(19)])\n",
    "        self.fichero = fichero\n",
    "\n",
    "        self.user = user_browsing()\n",
    "        self.user.set_standard_header(self.base)\n",
    "\n",
    "        response_content = self.get_bin_source_code()\n",
    "        self.html = response_content.decode(\"utf-8\")\n",
    "        self.soup = BeautifulSoup(response_content)\n",
    "\n",
    "        self.hyperlinks = self.find_hyperlinks()\n",
    "\n",
    "    def set_f1(self):\n",
    "        \"\"\"\n",
    "        Sets F1.\n",
    "        F1 = 1, if dots in url >= 4\n",
    "        F1 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        if self.url.count('.') >= 4:\n",
    "            self.fv[0] = 1\n",
    "\n",
    "        self.fv[0] = 0\n",
    "\n",
    "\n",
    "    def set_f2(self):\n",
    "        \"\"\"\n",
    "        Sets F2.\n",
    "        F2 = 1, if URL contains '@' or '-' symbols\n",
    "        F2 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        if '@' in self.url or '-' in self.url:\n",
    "            self.fv[1] = 1\n",
    "\n",
    "        self.fv[1] = 0\n",
    "\n",
    "\n",
    "    def set_f3(self):\n",
    "        \"\"\"\n",
    "        Sets F3.\n",
    "        F3 = 1, if URL length >= 74\n",
    "        F3 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.url) >= 74:\n",
    "            self.fv[2] = 1\n",
    "\n",
    "        self.fv[2] = 0\n",
    "\n",
    "\n",
    "    def set_f4(self):\n",
    "        \"\"\"\n",
    "        Sets F4.\n",
    "        F4 = 1, if URL contains any suspicious word\n",
    "        F4 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        splitted_url = self.get_splitted_url()\n",
    "        suspicious_words = self.get_suspicious_keywords()\n",
    "\n",
    "        for word in splitted_url:\n",
    "            if word in suspicious_words:\n",
    "                self.fv[3] = 1\n",
    "                break\n",
    "\n",
    "        self.fv[3] = 0\n",
    "\n",
    "\n",
    "    def get_splitted_url(self, type = 'full'):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if type == 'full':\n",
    "            url = self.url\n",
    "\n",
    "        else:\n",
    "            url = self.base\n",
    "\n",
    "        url = url.lower()\n",
    "        url = re.split('|\\/| ', url)\n",
    "\n",
    "        return url\n",
    "\n",
    "\n",
    "    def get_suspicious_keywords(self):\n",
    "        return ['security', 'login', 'signin', 'bank', 'account', 'update', 'include', 'webs', 'online']\n",
    "\n",
    "        \n",
    "    def set_f9(self):\n",
    "\n",
    "        try:\n",
    "\n",
    "            forms_found = re.findall(\"<form[^>]+>\", self.html)\n",
    "\n",
    "            if len(forms_found) > 0:\n",
    "\n",
    "                for i in range(len(forms_found)):\n",
    "\n",
    "                    form_found = forms_found[i]\n",
    "                    action_content = re.findall('(?:action=\\\")([^\"]*)(?:\\\")', form_found)\n",
    "                    print(action_content[0])\n",
    "\n",
    "                    if (len(action_content[0]) <1) or (len(re.findall('javascript:void\\(0\\)', action_content[0])) > 0):\n",
    "                        self.fv[8] = 1\n",
    "                        return\n",
    "\n",
    "                    elif len(re.findall('^(.*\\.php)$', action_content[0])) > 0 and self.base != self.process_url(action_content[0])[0]:\n",
    "                        self.fv[8] = 1\n",
    "                        return\n",
    "\n",
    "                    elif action_content[0][0] != '/' and self.base != self.process_url(action_content[0])[0]:\n",
    "                        self.fv[8] = 1\n",
    "                        return\n",
    "                    \n",
    "                    else:\n",
    "                        self.fv[8] = 0\n",
    "                    \n",
    "            else:\n",
    "                self.fv[8] = 0\n",
    "        \n",
    "        # No action found\n",
    "        except:\n",
    "            self.fv[8] = 0\n",
    "\n",
    "    \n",
    "    def set_f10_f11(self):\n",
    "\n",
    "        try:\n",
    "            n_hyperlinks_found = len(self.hyperlinks)\n",
    "            self.fv[9] = n_hyperlinks_found\n",
    "\n",
    "            if n_hyperlinks_found == 0:\n",
    "                self.fv[10] = 1\n",
    "\n",
    "            else:\n",
    "                self.fv[10] = 0\n",
    "\n",
    "        except:\n",
    "            self.fv[9] = 0\n",
    "            self.fv[10] = 0\n",
    "\n",
    "        \n",
    "    def set_f12(self):\n",
    "\n",
    "        if len(self.hyperlinks) < 1:\n",
    "            self.fv[11] = 1\n",
    "            return 0\n",
    "            \n",
    "        n_foreigns = self.get_number_foreign_hyperlinks() \n",
    "            \n",
    "        ratio = (n_foreigns / len(self.hyperlinks))\n",
    "\n",
    "        if ratio < 0.5:\n",
    "            self.fv[11] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[11] = 0\n",
    "\n",
    "            # for h in hyperlinks_found:\n",
    "            #     print(\"{} {} \".format(h, self.is_foreign(h)))\n",
    "\n",
    "        print(ratio)\n",
    "\n",
    "\n",
    "    def set_f13(self):\n",
    "\n",
    "        if len(self.hyperlinks) < 1:\n",
    "            self.fv[12] = 1\n",
    "            return\n",
    "            \n",
    "        n_empty = self.get_number_empty_hyperlinks()\n",
    "            \n",
    "        ratio = (n_empty / len(self.hyperlinks))\n",
    "\n",
    "        if ratio > 0.34:\n",
    "            self.fv[12] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[12] = 0\n",
    "\n",
    "        print(ratio)\n",
    "\n",
    "\n",
    "    def set_f14(self):\n",
    "\n",
    "        if len(self.hyperlinks) < 1:\n",
    "            self.fv[13] = 1\n",
    "            return\n",
    "            \n",
    "        n_errors = self.get_number_errors()\n",
    "            \n",
    "        ratio = (n_errors / len(self.hyperlinks))\n",
    "\n",
    "        if ratio > 0.3:\n",
    "            self.fv[13] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[13] = 0\n",
    "\n",
    "        print(ratio)\n",
    "\n",
    "    \n",
    "    def set_f15(self):\n",
    "\n",
    "        if len(self.hyperlinks) < 1:\n",
    "            self.fv[14] = 1\n",
    "            return\n",
    "            \n",
    "        n_redirects = self.get_number_redirects()\n",
    "            \n",
    "        ratio = (n_redirects / len(self.hyperlinks))\n",
    "\n",
    "        if ratio > 0.3:\n",
    "            self.fv[14] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[14] = 0\n",
    "\n",
    "        print(ratio)\n",
    "\n",
    "\n",
    "    def set_f16(self):\n",
    "        \"\"\"\n",
    "        Sets F16.\n",
    "        F16 = 1, if CSS file is external and contains foreign domain name\n",
    "        F16 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        external_csss = self.soup.findAll(\"link\", rel=\"stylesheet\")\n",
    "\n",
    "        for css in external_csss:\n",
    "\n",
    "            link = self.extract_url_href(css)\n",
    "            \n",
    "            if self.is_foreign(link):\n",
    "                self.fv[15] = 1\n",
    "                break\n",
    "\n",
    "        self.fv[15] = 0 \n",
    "\n",
    "\n",
    "    def set_f17(self):\n",
    "\n",
    "        copyright_clues = ['Â©', '& copy', 'copy', 'copyright', 'copyright', 'all right reserved', 'rights', 'right'] #'@', \n",
    "\n",
    "        base_domain = self.base.split(\".\")\n",
    "\n",
    "        for clue in copyright_clues:\n",
    "\n",
    "            regex = '(?:{})([^\"]*)(?:[\\.\\\"])'.format(clue)\n",
    "            copy_contents = re.findall(regex, self.html)\n",
    "\n",
    "            for copy_content in copy_contents:\n",
    "\n",
    "                copy_content = copy_content.replace(\" \", \"\")\n",
    "\n",
    "                for base in base_domain:\n",
    "                    if re.search(base, copy_content, re.IGNORECASE):\n",
    "                        self.fv[16] = 0\n",
    "                        return\n",
    "        \n",
    "        self.fv[16] = 1\n",
    "\n",
    "\n",
    "    def set_f18(self):\n",
    "        \n",
    "        keywords = self.get_site_keywords()\n",
    "        #base = re.match('(:?(www.)?)([^.]*)(:?.[A-Za-z]{0,4})', self.base)[0]\n",
    "        #base = base.split('.')\n",
    "\n",
    "        for keyword in keywords:\n",
    "\n",
    "            if re.findall(keyword, self.base):\n",
    "                self.fv[17] = 0\n",
    "                break\n",
    "        \n",
    "        self.fv[17] = 1\n",
    "\n",
    "\n",
    "    def set_f19(self):\n",
    "        \"\"\"\n",
    "        Sets F19.\n",
    "        F19 = 1, if foreign domain found in favicon link\n",
    "        F19 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        icons = self.soup.findAll(\"link\", rel=\"icon\") + self.soup.findAll(\"link\", rel=\"shortcut icon\")\n",
    "\n",
    "        for icon in icons:\n",
    "\n",
    "            link = self.extract_url_href(icon)\n",
    "\n",
    "            if self.is_foreign(link):\n",
    "                self.fv[18] = 1\n",
    "                print(1)\n",
    "                break\n",
    "\n",
    "        self.fv[18] = 0 \n",
    "\n",
    "\n",
    "    def process_url(self, url):\n",
    "\n",
    "        parsed = urlparse(url)\n",
    "        base = parsed.netloc\n",
    "        return base, base + '/'.join(path.split('/')[:-1])\n",
    "\n",
    "\n",
    "    def get_bin_source_code(self):\n",
    "\n",
    "        response = requests.get(self.url, headers=self.user.get_simple_user_header_agent() ) #, headers=self.user.header) #proxies=user.proxies, cookies=user.cookies)\n",
    "\n",
    "        if response.status_code != 400:\n",
    "            with open(self.fichero, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "                f.close()\n",
    "\n",
    "        return response.content\n",
    "\n",
    "    def get_title(self):\n",
    "        return re.findall('(?:<title>)([^<]*)(?:</title>)', self.html)\n",
    "\n",
    "\n",
    "    def get_meta(self):\n",
    "\n",
    "        keywords = []\n",
    "        found = re.findall('(?:<meta)([^>]*)(?:>)', self.html) #(?:<meta.*content=\")([^\"]*)(?:\")\n",
    "\n",
    "        for content in found:\n",
    "            match = re.findall('(?:content=\")([^\"]*)(?:\")', content)\n",
    "\n",
    "            if len(match) > 0:\n",
    "                keywords.append(match[0])\n",
    "\n",
    "        return keywords\n",
    "\n",
    "\n",
    "    def get_meta_title_words(self):\n",
    "\n",
    "        list = self.get_title() + self.get_meta()\n",
    "        words = ' '.join(list)\n",
    "\n",
    "        return Utils().preprocess(words)\n",
    "\n",
    "\n",
    "    def find_hyperlinks(self):\n",
    "        return ( re.findall('(?:src\\b*=\\b*\\\")([^\"]*)(?:\\\")', self.html) + re.findall('(?:href\\b*=\\b*\\\")([^\"]*)(?:\\\")', self.html) )\n",
    "    \n",
    "    def url_validator(self, url):\n",
    "        parsed = urlparse(url)\n",
    "        return parsed.netloc != ''\n",
    "\n",
    "\n",
    "    def extract_url_href(self, tag):\n",
    "        \"\"\"\n",
    "        Returns url included inside href atribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tag : str\n",
    "            Tag containing the href atribute.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            extracted link (empty if none)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        matches = re.findall('(?:href=\")([^\"]*)(?:\")', str(tag))\n",
    "\n",
    "        if len(matches) > 0:\n",
    "            return matches[0]\n",
    "        \n",
    "        return ''\n",
    "\n",
    "\n",
    "    def get_response_code(self, url):\n",
    "        return requests.get(url).status_code\n",
    "\n",
    "\n",
    "    def get_number_foreign_hyperlinks(self):\n",
    "\n",
    "        n_foreigns = 0\n",
    "\n",
    "        for h in self.hyperlinks:\n",
    "            if self.is_foreign(h):\n",
    "                n_foreigns += 1\n",
    "\n",
    "        return n_foreigns\n",
    "\n",
    "    def get_number_empty_hyperlinks(self):\n",
    "\n",
    "        n_empty = 0\n",
    "\n",
    "        for h in self.hyperlinks:\n",
    "            if self.is_empty(h):\n",
    "                n_empty += 1\n",
    "\n",
    "        return n_empty\n",
    "\n",
    "\n",
    "    def is_absolute(self, url):\n",
    "        return bool(urlparse(url).netloc)\n",
    "\n",
    "    def is_empty(self, url):\n",
    "        return url[0] == '#' or bool(re.match('[Jj]ava[Ss]cript::?void\\(0\\)', url))\n",
    "\n",
    "    def is_relative_in_local(self, url):\n",
    "\n",
    "        if self.is_absolute(url):\n",
    "            return False\n",
    "\n",
    "        pattern = re.compile(\"^[/]?[A-z0-9_]\")\n",
    "        return bool (pattern.match(url))\n",
    "        \n",
    "    \n",
    "    def get_number_errors(self):\n",
    "\n",
    "        n_errors = 0\n",
    "\n",
    "        for h in self.hyperlinks:\n",
    "\n",
    "            if not self.is_empty(h) and not self.is_relative_in_local(h):\n",
    "                code = self.get_response_code(h)\n",
    "\n",
    "                if code == 404 or code == 403:\n",
    "                    n_errors += 1\n",
    "\n",
    "        return n_errors\n",
    "\n",
    "\n",
    "    def get_number_redirects(self):\n",
    "\n",
    "        n_redirects = 0\n",
    "\n",
    "        for h in self.hyperlinks:\n",
    "\n",
    "            if not self.is_empty(h) and not self.is_relative_in_local(h):\n",
    "                code = self.get_response_code(h)\n",
    "\n",
    "                if code == 302 or code == 301:\n",
    "                    n_redirects += 1\n",
    "\n",
    "        return n_redirects\n",
    "\n",
    "\n",
    "    def is_foreign(self, url):\n",
    "        return not self.is_relative_in_local(url) and not self.is_empty(url) and self.base != urlparse(url).netloc\n",
    "\n",
    "    def get_popular_words(self, k=10):\n",
    "\n",
    "        cleaned = BeautifulSoup(self.html, \"lxml\").text\n",
    "        tokens = Utils().preprocess(cleaned)\n",
    "        counter = Counter(tokens)\n",
    "        n_words = len(tokens)\n",
    "\n",
    "        for token in np.unique(tokens):\n",
    "        \n",
    "            tf = counter[token]/n_words\n",
    "            #df = doc_freq(token)\n",
    "            #idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        #tf_idf[doc, token] = tf*idf\n",
    "\n",
    "        return counter.most_common(k)\n",
    "\n",
    "    def get_site_keywords(self):\n",
    "        \n",
    "        set_one = set(self.get_meta_title_words())\n",
    "        set_two = set([word[0] for word in self.get_popular_words()])\n",
    "\n",
    "        return set_one.union(set_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('https://www.naturaselection.com/es/') \n",
    "# ('https://www.bershka.com/es/h-woman.html') \n",
    "# ('https://ubuvirtual.ubu.es/') \n",
    "# ('https://fdeageadfahgeafeahg.azurewebsites.net/renner/inicio/login.php')\n",
    "# ('https://banrural.herokuapp.com/')\n",
    "\n",
    "ph_entity = PHISH_FVG('https://ubuvirtual.ubu.es/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ph_entity.set_f9()\n",
    "\n",
    "# ph_entity.set_f10_f11()\n",
    "\n",
    "\n",
    "# print(ph_entity.is_foreign('https://www.ubuvirtual.com/image.jpg'))\n",
    "# print(ph_entity.is_foreign('www.fdeageadfahgeafeahg.azurewebsites.net/renner/inicio/login.php'))\n",
    "\n",
    "# ph_entity.set_f12()\n",
    "# ph_entity.set_f13()\n",
    "#ph_entity.set_f17()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_entity.set_f1()\n",
    "ph_entity.set_f2()\n",
    "ph_entity.set_f3()\n",
    "ph_entity.set_f4()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TESTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 1.220s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class RealFV(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        self.ph_entity = PHISH_FVG('https://ubuvirtual.ubu.es/')\n",
    "\n",
    "    def test_correct_initialize(self):\n",
    "        self.assertTrue(np.array(self.ph_entity.fv).sum() == -19)\n",
    "\n",
    "    def test_f1(self):\n",
    "        self.assertTrue(self.ph_entity.fv[0] == -1)\n",
    "        self.ph_entity.set_f1()\n",
    "        self.assertTrue(self.ph_entity.fv[0] == 0)\n",
    "\n",
    "    def test_f2(self):\n",
    "        self.assertTrue(self.ph_entity.fv[1] == -1)\n",
    "        self.ph_entity.set_f2()\n",
    "        self.assertTrue(self.ph_entity.fv[1] == 0)\n",
    "\n",
    "    def test_f3(self):\n",
    "        self.assertTrue(self.ph_entity.fv[2] == -1)\n",
    "        self.ph_entity.set_f3()\n",
    "        self.assertTrue(self.ph_entity.fv[2] == 0)\n",
    "        \n",
    "\n",
    "class phishingUtilsMethods(unittest.TestCase):\n",
    "\n",
    "    ut = Utils()\n",
    "\n",
    "    def test_translate_leet(self):\n",
    "\n",
    "        phishing_words = ['l0g1n', '13urg05', '5h0pp1ng', '4maz0n']\n",
    "        real_words = ['login', 'burgos', 'shopping', 'amazon']\n",
    "\n",
    "        for phish, real in zip(phishing_words, real_words):\n",
    "            alternatives = self.ut.translate_leet_to_letters(phish)\n",
    "            self.assertTrue(real in alternatives)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5061e2ebcf242305dcdb45d871ef5cd4ba433365f314bc2418d41c77ce076e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
