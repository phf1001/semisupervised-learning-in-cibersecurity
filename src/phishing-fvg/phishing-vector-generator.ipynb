{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PHISHING VECTOR GENERATOR** üêü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\patri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from user_browsing import user_browsing\n",
    "from xml.etree import ElementTree as ET\n",
    "from urllib.parse import urlparse\n",
    "from os import path\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "from phishing_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PHISH_FVG:\n",
    "\n",
    "    def __init__(self, url, fichero = 'html_dump'):\n",
    "\n",
    "        self.url = url\n",
    "        parsed = urlparse(url)\n",
    "        self.base = parsed.netloc\n",
    "        self.path = self.base + '/'.join(path.split('/')[:-1])\n",
    "\n",
    "        self.fv = np.array([-1 for i in range(19)])\n",
    "        self.fichero = fichero\n",
    "\n",
    "        self.user = user_browsing()\n",
    "        self.user.set_standard_header(self.base)\n",
    "\n",
    "        response_content = self.get_bin_source_code()\n",
    "        self.html = response_content.decode(\"utf-8\")\n",
    "        self.title = re.findall('(?:<title>)([^<]*)(?:</title>)', self.html)\n",
    "        self.soup = BeautifulSoup(response_content)\n",
    "\n",
    "        self.hyperlinks = self.find_hyperlinks()\n",
    "        \n",
    "\n",
    "    def get_bin_source_code(self):\n",
    "        \"\"\"\n",
    "        Extracts binary source code from webpage.\n",
    "        \"\"\"\n",
    "\n",
    "        response = requests.get(self.url, headers=self.user.get_simple_user_header_agent() ) #, headers=self.user.header) #proxies=user.proxies, cookies=user.cookies)\n",
    "\n",
    "        if response.status_code != 400:\n",
    "            with open(self.fichero, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "                f.close()\n",
    "\n",
    "        return response.content\n",
    "\n",
    "        \n",
    "    def set_f1(self):\n",
    "        \"\"\"\n",
    "        Sets F1.\n",
    "        F1 = 1, if dots in url >= 4\n",
    "        F1 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        if self.url.count('.') >= 4:\n",
    "            self.fv[0] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[0] = 0\n",
    "\n",
    "\n",
    "    def set_f2(self):\n",
    "        \"\"\"\n",
    "        Sets F2.\n",
    "        F2 = 1, if URL contains '@' or '-' symbols\n",
    "        F2 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        if '@' in self.url or '-' in self.url:\n",
    "            self.fv[1] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[1] = 0\n",
    "\n",
    "\n",
    "    def set_f3(self):\n",
    "        \"\"\"\n",
    "        Sets F3.\n",
    "        F3 = 1, if URL length >= 74\n",
    "        F3 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.url) >= 74:\n",
    "            self.fv[2] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[2] = 0\n",
    "\n",
    "\n",
    "    def set_f4(self):\n",
    "        \"\"\"\n",
    "        Sets F4.\n",
    "        F4 = 1, if URL contains any suspicious word\n",
    "        F4 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        splitted_url = get_splitted_url(self.url)\n",
    "        suspicious_words = get_suspicious_keywords()\n",
    "\n",
    "        for word in splitted_url:\n",
    "            leet_translation = translate_leet_to_letters(word) #Decisi√≥n propia\n",
    "            \n",
    "            if bool(suspicious_words & leet_translation):\n",
    "                self.fv[3] = 1\n",
    "                return\n",
    "\n",
    "        self.fv[3] = 0\n",
    "\n",
    "\n",
    "    def set_f5(self):\n",
    "        \"\"\"\n",
    "        Sets F5.\n",
    "        F5 = 1, if tlds in URL > 1\n",
    "        F5 = 0, otherwise\n",
    "\n",
    "        # REVISAR COMPUESTOS\n",
    "        \"\"\"\n",
    "\n",
    "        splitted_url = set(get_splitted_url(self.url))\n",
    "        tlds = get_tlds_set()\n",
    "\n",
    "        if len(splitted_url & tlds) > 1:\n",
    "            self.fv[4] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[4] = 0\n",
    "\n",
    "\n",
    "    def set_f6(self):\n",
    "        \"\"\"\n",
    "        Sets F6.\n",
    "        F6 = 1, if http count in URL > 1\n",
    "        F6 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        if len(re.findall('http', self.url)) > 1:\n",
    "            self.fv[5] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[5] = 0\n",
    "\n",
    "\n",
    "    def set_f7(self):\n",
    "        \"\"\"\n",
    "        Sets F7.\n",
    "        F7 = 1, if brand in incorrect position.\n",
    "        F7 = 0, otherwise\n",
    "        # Unitarias\n",
    "        \"\"\"\n",
    "\n",
    "        targets = get_phishing_targets_set()\n",
    "        parsed = urlparse(self.url.lower())\n",
    "        base = remove_tld(parsed.netloc)\n",
    "        base = remove_tld(base)\n",
    "        path = parsed.path\n",
    "\n",
    "        for target in targets:\n",
    "            if target in base or target in path:\n",
    "                self.fv[6] = 1\n",
    "                return\n",
    "\n",
    "        self.fv[6] = 0\n",
    "\n",
    "\n",
    "    def set_f8(self):\n",
    "        \"\"\"\n",
    "        Sets F8.\n",
    "        F8 = 1, if data URI present in website.\n",
    "        F8 = 0, otherwise\n",
    "\n",
    "        Syntax: data:[<mime type>][;charset=<charset>][;base64],<encoded data>\n",
    "        \"\"\"\n",
    "\n",
    "        matches = re.findall('data:(?:[^;,]*)?(?:;charset=[^;,]*)?(?:;base64)?,[^)\"\\';>]*[^)\"\\';>]', self.html)\n",
    "        print(matches)\n",
    "        \n",
    "        if len(matches) > 0:\n",
    "            self.fv[7] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[7] = 0\n",
    "            \n",
    "\n",
    "    def set_f9(self):\n",
    "        \"\"\"\n",
    "        Sets F9.\n",
    "        F9 = 1, if action field is blank or javascript:void(0)\n",
    "        F9 = 1, if action field is <name>.php\n",
    "        F9 = 1, if action field contains foreign base domain\n",
    "        F9 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        forms_found = re.findall(\"<form[^>]+>\", self.html)\n",
    "\n",
    "        if len(forms_found) > 0:\n",
    "\n",
    "            for i in range(len(forms_found)):\n",
    "                form_found = forms_found[i]\n",
    "                action_content = re.findall('(?:action=\\\")([^\"]*)(?:\\\")', form_found)\n",
    "\n",
    "                if len(action_content) > 0:\n",
    "\n",
    "                    if is_empty(action_content[0]):\n",
    "                        self.fv[8] = 1\n",
    "                        return\n",
    "\n",
    "                    elif is_simple_php_file(action_content[0]):\n",
    "                        self.fv[8] = 1\n",
    "                        return\n",
    "\n",
    "                    elif is_foreign(self.url, action_content[0]):\n",
    "                        self.fv[8] = 1\n",
    "                        return\n",
    "                        \n",
    "        self.fv[8] = 0\n",
    "\n",
    "\n",
    "    def set_f10_f11(self):\n",
    "        \"\"\"\n",
    "        Sets F10 and F11.\n",
    "\n",
    "        F10 = number of hyperlinks in source code.\n",
    "\n",
    "        F11 = 1, if no hyperlinks found in source.\n",
    "        F11 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        n_hyperlinks_found = len(self.hyperlinks)\n",
    "        self.fv[9] = n_hyperlinks_found\n",
    "\n",
    "        if n_hyperlinks_found == 0:\n",
    "            self.fv[10] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[10] = 0\n",
    "\n",
    "\n",
    "    def set_f12(self):\n",
    "        \"\"\"\n",
    "        Sets F12.\n",
    "\n",
    "        ratio = |n_foreign_hyp| / |n_hyp|\n",
    "\n",
    "        F12 = 1 if ratio > 0.5 and n_hyp > 0\n",
    "        F12 = 0 otherwise\n",
    "\n",
    "        REVISAR\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.hyperlinks) < 1:\n",
    "            self.fv[11] = 1 # Deber√≠a ser 0 pero es mejor 1 ya que es phishing clar√≠simamente\n",
    "            return\n",
    "            \n",
    "        n_foreigns = self.get_number_foreign_hyperlinks() \n",
    "        ratio = (n_foreigns / len(self.hyperlinks))\n",
    "\n",
    "        if ratio > 0.5:\n",
    "            self.fv[11] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[11] = 0\n",
    "\n",
    "\n",
    "    def set_f13(self):\n",
    "        \"\"\"\n",
    "        Sets F13.\n",
    "\n",
    "        ratio = |n_empty_hyp| / |n_hyp|\n",
    "\n",
    "        F13 = 1 if ratio > 0.34 and n_hyp > 0\n",
    "        F13 = 0 otherwise\n",
    "\n",
    "        REVISAR\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.hyperlinks) < 1:\n",
    "            self.fv[12] = 1 # Deber√≠a ser 0 pero es mejor 1 ya que es phishing clar√≠simamente\n",
    "            return\n",
    "            \n",
    "        n_empty = self.get_number_empty_hyperlinks()\n",
    "        ratio = (n_empty / len(self.hyperlinks))\n",
    "\n",
    "        if ratio > 0.34:\n",
    "            self.fv[12] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[12] = 0\n",
    "\n",
    "\n",
    "    #------------------------------------REVISANDO Y MOVIENDO A LOS UTILS-----------------------------------------------------\n",
    "\n",
    "    def set_f14(self):\n",
    "\n",
    "        if len(self.hyperlinks) < 1:\n",
    "            self.fv[13] = 1\n",
    "            return\n",
    "            \n",
    "        n_errors = self.get_number_errors()\n",
    "            \n",
    "        ratio = (n_errors / len(self.hyperlinks))\n",
    "\n",
    "        if ratio > 0.3:\n",
    "            self.fv[13] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[13] = 0\n",
    "\n",
    "        print(ratio)\n",
    "\n",
    "    \n",
    "    def set_f15(self):\n",
    "\n",
    "        if len(self.hyperlinks) < 1:\n",
    "            self.fv[14] = 1\n",
    "            return\n",
    "            \n",
    "        n_redirects = self.get_number_redirects()\n",
    "            \n",
    "        ratio = (n_redirects / len(self.hyperlinks))\n",
    "\n",
    "        if ratio > 0.3:\n",
    "            self.fv[14] = 1\n",
    "\n",
    "        else:\n",
    "            self.fv[14] = 0\n",
    "\n",
    "        print(ratio)\n",
    "\n",
    "\n",
    "    def set_f16(self):\n",
    "        \"\"\"\n",
    "        Sets F16.\n",
    "        F16 = 1, if CSS file is external and contains foreign domain name\n",
    "        F16 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        external_csss = self.soup.findAll(\"link\", rel=\"stylesheet\")\n",
    "\n",
    "        for css in external_csss:\n",
    "\n",
    "            link = self.extract_url_href(css)\n",
    "            \n",
    "            if is_foreign(link):\n",
    "                self.fv[15] = 1\n",
    "                break\n",
    "\n",
    "        self.fv[15] = 0 \n",
    "\n",
    "\n",
    "    def set_f17(self):\n",
    "\n",
    "        copyright_clues = ['¬©', '& copy', 'copy', 'copyright', 'copyright', 'all right reserved', 'rights', 'right'] #'@', \n",
    "\n",
    "        base_domain = self.base.split(\".\")\n",
    "\n",
    "        for clue in copyright_clues:\n",
    "\n",
    "            regex = '(?:{})([^\"]*)(?:[\\.\\\"])'.format(clue)\n",
    "            copy_contents = re.findall(regex, self.html)\n",
    "\n",
    "            for copy_content in copy_contents:\n",
    "\n",
    "                copy_content = copy_content.replace(\" \", \"\")\n",
    "\n",
    "                for base in base_domain:\n",
    "                    if re.search(base, copy_content, re.IGNORECASE):\n",
    "                        self.fv[16] = 0\n",
    "                        return\n",
    "        \n",
    "        self.fv[16] = 1\n",
    "\n",
    "\n",
    "    def set_f18(self):\n",
    "        \n",
    "        keywords = self.get_site_keywords()\n",
    "        #base = re.match('(:?(www.)?)([^.]*)(:?.[A-Za-z]{0,4})', self.base)[0]\n",
    "        #base = base.split('.')\n",
    "\n",
    "        for keyword in keywords:\n",
    "\n",
    "            if re.findall(keyword, self.base):\n",
    "                self.fv[17] = 0\n",
    "                break\n",
    "        \n",
    "        self.fv[17] = 1\n",
    "\n",
    "\n",
    "    def set_f19(self):\n",
    "        \"\"\"\n",
    "        Sets F19.\n",
    "        F19 = 1, if foreign domain found in favicon link\n",
    "        F19 = 0, otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        icons = self.soup.findAll(\"link\", rel=\"icon\") + self.soup.findAll(\"link\", rel=\"shortcut icon\")\n",
    "\n",
    "        for icon in icons:\n",
    "\n",
    "            link = self.extract_url_href(icon)\n",
    "\n",
    "            if self.is_foreign(link):\n",
    "                self.fv[18] = 1\n",
    "                print(1)\n",
    "                break\n",
    "\n",
    "        self.fv[18] = 0 \n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def get_meta(self):\n",
    "\n",
    "        keywords = []\n",
    "        found = re.findall('(?:<meta)([^>]*)(?:>)', self.html) #(?:<meta.*content=\")([^\"]*)(?:\")\n",
    "\n",
    "        for content in found:\n",
    "            match = re.findall('(?:content=\")([^\"]*)(?:\")', content)\n",
    "\n",
    "            if len(match) > 0:\n",
    "                keywords.append(match[0])\n",
    "\n",
    "        return keywords\n",
    "\n",
    "\n",
    "    def get_meta_title_words(self):\n",
    "\n",
    "        list = self.title + self.get_meta()\n",
    "        words = ' '.join(list)\n",
    "\n",
    "        return preprocess(words)\n",
    "\n",
    "\n",
    "    def find_hyperlinks(self):\n",
    "        \"\"\"\n",
    "        Finds number of pages in a website extracting them\n",
    "        from the src attribute and href attribute of anchor\n",
    "        tags.\n",
    "        \"\"\"\n",
    "        return ( re.findall('(?:src\\b*=\\b*\\\")([^\"]*)(?:\\\")', self.html) + re.findall('(?:href\\b*=\\b*\\\")([^\"]*)(?:\\\")', self.html) )\n",
    "    \n",
    "    def url_validator(self, url):\n",
    "        parsed = urlparse(url)\n",
    "        return parsed.netloc != ''\n",
    "\n",
    "\n",
    "    def get_response_code(self, url):\n",
    "        return requests.get(url).status_code\n",
    "\n",
    "\n",
    "    def get_number_foreign_hyperlinks(self):\n",
    "        \"\"\"\n",
    "        Returns the number of foreign hyperlinks\n",
    "        \"\"\"\n",
    "\n",
    "        n_foreigns = 0\n",
    "\n",
    "        for h in self.hyperlinks:\n",
    "            if is_foreign(h):\n",
    "                n_foreigns += 1\n",
    "\n",
    "        return n_foreigns\n",
    "\n",
    "    def get_number_empty_hyperlinks(self):\n",
    "\n",
    "        n_empty = 0\n",
    "\n",
    "        for h in self.hyperlinks:\n",
    "            if self.is_empty(h):\n",
    "                n_empty += 1\n",
    "\n",
    "        return n_empty\n",
    "        \n",
    "    \n",
    "    def get_number_errors(self):\n",
    "\n",
    "        n_errors = 0\n",
    "\n",
    "        for h in self.hyperlinks:\n",
    "\n",
    "            if not self.is_empty(h) and not self.is_relative_in_local(h):\n",
    "                code = self.get_response_code(h)\n",
    "\n",
    "                if code == 404 or code == 403:\n",
    "                    n_errors += 1\n",
    "\n",
    "        return n_errors\n",
    "\n",
    "\n",
    "    def get_number_redirects(self):\n",
    "\n",
    "        n_redirects = 0\n",
    "\n",
    "        for h in self.hyperlinks:\n",
    "\n",
    "            if not self.is_empty(h) and not self.is_relative_in_local(h):\n",
    "                code = self.get_response_code(h)\n",
    "\n",
    "                if code == 302 or code == 301:\n",
    "                    n_redirects += 1\n",
    "\n",
    "        return n_redirects\n",
    "\n",
    "\n",
    "    def get_popular_words(self, k=10):\n",
    "\n",
    "        cleaned = BeautifulSoup(self.html, \"lxml\").text\n",
    "        tokens = preprocess(cleaned)\n",
    "        counter = Counter(tokens)\n",
    "        n_words = len(tokens)\n",
    "\n",
    "        for token in np.unique(tokens):\n",
    "        \n",
    "            tf = counter[token]/n_words\n",
    "            #df = doc_freq(token)\n",
    "            #idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        #tf_idf[doc, token] = tf*idf\n",
    "\n",
    "        return counter.most_common(k)\n",
    "\n",
    "    def get_site_keywords(self):\n",
    "        \n",
    "        set_one = set(self.get_meta_title_words())\n",
    "        set_two = set([word[0] for word in self.get_popular_words()])\n",
    "\n",
    "        return set_one.union(set_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('https://www.naturaselection.com/es/') \n",
    "# ('https://www.bershka.com/es/h-woman.html') \n",
    "# ('https://ubuvirtual.ubu.es/') \n",
    "# ('https://fdeageadfahgeafeahg.azurewebsites.net/renner/inicio/login.php')\n",
    "# ('https://banrural.herokuapp.com/')\n",
    "# ('http://w3.unpocodetodo.info/canvas/data_uri.php')\n",
    "# ph_entity = PHISH_FVG('https://www.facebook.com/login/')\n",
    "# ph_entity.set_f4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ph_entity.set_f9()\n",
    "\n",
    "# ph_entity.set_f10_f11()\n",
    "\n",
    "\n",
    "# print(ph_entity.is_foreign('https://www.ubuvirtual.com/image.jpg'))\n",
    "# print(ph_entity.is_foreign('www.fdeageadfahgeafeahg.azurewebsites.net/renner/inicio/login.php'))\n",
    "\n",
    "# ph_entity.set_f12()\n",
    "# ph_entity.set_f13()\n",
    "#ph_entity.set_f17()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAGCAQAAACh8pzAAAAADklEQVR42mNkgANG8pgAASkAB1mKT2kAAAAASUVORK5CYII=', 'data:[{},{}],fetch:[],error:e,state:{']\n"
     ]
    }
   ],
   "source": [
    "ph_entity = PHISH_FVG('https://www.bershka.com/es/h-woman.html') \n",
    "\n",
    "ph_entity.set_f8()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TESTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "........\n",
      "----------------------------------------------------------------------\n",
      "Ran 8 tests in 0.007s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# class RealFV(unittest.TestCase):\n",
    "    \n",
    "#     def setUp(self):\n",
    "#         self.ph_entity = PHISH_FVG('https://ubuvirtual.ubu.es/')\n",
    "\n",
    "#     def test_correct_initialize(self):\n",
    "#         self.assertTrue(np.array(self.ph_entity.fv).sum() == -19)\n",
    "\n",
    "#     def test_f1(self):\n",
    "#         self.assertTrue(self.ph_entity.fv[0] == -1)\n",
    "#         self.ph_entity.set_f1()\n",
    "#         self.assertTrue(self.ph_entity.fv[0] == 0)\n",
    "\n",
    "#     def test_f2(self):\n",
    "#         self.assertTrue(self.ph_entity.fv[1] == -1)\n",
    "#         self.ph_entity.set_f2()\n",
    "#         self.assertTrue(self.ph_entity.fv[1] == 0)\n",
    "\n",
    "#     def test_f3(self):\n",
    "#         self.assertTrue(self.ph_entity.fv[2] == -1)\n",
    "#         self.ph_entity.set_f3()\n",
    "#         self.assertTrue(self.ph_entity.fv[2] == 0)\n",
    "        \n",
    "\n",
    "class phishingUtilsMethods(unittest.TestCase):\n",
    "\n",
    "    def test_translate_leet(self):\n",
    "\n",
    "        phishing_words = ['l0g1n', '13urg05', '5h0pp1ng', '4maz0n', 'm1crosoft']\n",
    "        real_words = ['login', 'burgos', 'shopping', 'amazon', 'microsoft']\n",
    "\n",
    "        for phish, real in zip(phishing_words, real_words):\n",
    "            alternatives = translate_leet_to_letters(phish)\n",
    "            self.assertTrue(real in alternatives)\n",
    "\n",
    "    \n",
    "    def test_split_url(self):\n",
    "        urls = ['https://ubuvirtual.ubu.es/', 'www.ubu-virtual.ubu.es/ruta/archivo.php']\n",
    "        splitted_urls = [['https', 'ubuvirtual', 'ubu', 'es'], ['www', 'ubu', 'virtual', 'ubu', 'es', 'ruta', 'archivo', 'php']]\n",
    "\n",
    "        for input_test, output_test in zip(urls, splitted_urls):\n",
    "            result = get_splitted_url(input_test)\n",
    "            self.assertTrue(result == output_test)\n",
    "\n",
    "\n",
    "    def test_tlds_set(self):\n",
    "        \n",
    "        tlds = get_tlds_set()\n",
    "        self.assertTrue(len(tlds) == 150)\n",
    "        self.assertTrue(bool(tlds & {'com'}))\n",
    "        self.assertTrue(bool(tlds & {'es'}))\n",
    "        self.assertTrue(bool(tlds & {'edu'}))\n",
    "        self.assertTrue(bool(tlds & {'fr'}))\n",
    "        self.assertTrue(bool(tlds & {'org'}))\n",
    "\n",
    "\n",
    "    def test_targets_set(self):\n",
    "        \n",
    "        tlds = get_phishing_targets_set()\n",
    "        self.assertTrue(bool(tlds & {'amazon'}))\n",
    "        self.assertTrue(bool(tlds & {'dropbox'}))\n",
    "        self.assertTrue(bool(tlds & {'azure'}))\n",
    "        self.assertTrue(bool(tlds & {'linkedin'}))\n",
    "        self.assertTrue(bool(tlds & {'correos'}))\n",
    "\n",
    "\n",
    "    def test_remove_tld(self):\n",
    "        urls = ['ubuvirtual.ubu.es.org.uk', 'ubuvirtual.ubu.es.org', 'ubuvirtual.ubu.es']\n",
    "        without_tlds = ['ubuvirtual.ubu.es.org', 'ubuvirtual.ubu.es', 'ubuvirtual.ubu']\n",
    "\n",
    "        for input_test, output_test in zip(urls, without_tlds):\n",
    "            result = remove_tld(input_test)\n",
    "            self.assertTrue(result == output_test)\n",
    "\n",
    "\n",
    "    def test_empty_content(self):\n",
    "        \n",
    "        empty = ['#', 'javascript:void(0)', '']\n",
    "        not_empty = ['something', '/unexpected']\n",
    "\n",
    "        for input_test in empty:\n",
    "            self.assertTrue(is_empty(input_test))\n",
    "\n",
    "        for input_test in not_empty:\n",
    "            self.assertFalse(is_empty(input_test))\n",
    "\n",
    "\n",
    "    def test_simple_php_file(self):\n",
    "        \n",
    "        simple = ['index.php', 'login.php', 'mail.php']\n",
    "        not_simple = ['/index.php', 'something.something.php']\n",
    "\n",
    "        for input_test in simple:\n",
    "            self.assertTrue(is_simple_php_file(input_test))\n",
    "\n",
    "        for input_test in not_simple:\n",
    "            self.assertFalse(is_simple_php_file(input_test))\n",
    "\n",
    "\n",
    "    def test_domains(self):\n",
    "        \n",
    "        base = 'https://ubuvirtual.ubu.es/'\n",
    "        absolute = ['https://pwr.edu.pl/', 'https://www.uc3m.es/Inicio', 'https://estudios.uoc.edu/es/estudiar-online']\n",
    "        relative = ['/mail.php', '/image/ruta/inventada.jpg', 'hola.html']\n",
    "\n",
    "        for input_test in absolute:\n",
    "            self.assertTrue(is_absolute(input_test))\n",
    "            self.assertTrue(is_foreign(base, input_test))\n",
    "\n",
    "        for input_test in relative:\n",
    "            self.assertFalse(is_absolute(input_test))\n",
    "            self.assertTrue(is_relative_in_local(input_test))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5061e2ebcf242305dcdb45d871ef5cd4ba433365f314bc2418d41c77ce076e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
