{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **游냀 CO-FOREST PYTONIZADO** \n",
    "\n",
    "##### **Autora: Patricia Hernando Fern치ndez**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DEBUGGEAR (domingo):*\n",
    "\n",
    "Problema: oob error\n",
    "\n",
    "* Si todos los oob error de la primera ronda son 1 (no se vota), se ignora el entrenamiento. Sustitu칤do por nan si no se vota.\n",
    "쯈u칠 hacer con nan? Deber칤a arreglarse autom치ticamente si aumenta mucho el n칰mero de datos y de 치rboles creo.\n",
    "\n",
    "Problema: WMax cuando el error de la iteraci칩n es 0\n",
    "\n",
    "* Se ha limitado a un Wmax m치ximo de 100, pero habr칤a que experimentar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "### **IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CLASS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class noVotesException(Exception):\n",
    "\n",
    "    def __init__(self, message=\"No tree voted\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Co_Forest:\n",
    "    \"\"\"\"\n",
    "    Class used to generate a semi-supervised Random Forest.\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    ...\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    ...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, L, y, U, n, sigma, classes, max_features='sqrt'):\n",
    "        \"\"\"\"\n",
    "        Constructor. Creates and trains the Co-Forest.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        L: np.array\n",
    "            labeled data used for training\n",
    "        y: np.array\n",
    "            tags of the labeled data used for training\n",
    "        U: np.array\n",
    "            unlabeled data used for training\n",
    "        n: int\n",
    "            number of trees in the ensemble\n",
    "        sigma: float\n",
    "            tolerance\n",
    "        classes: np.array\n",
    "            names of the classes that can be predicted\n",
    "        max_features: string\n",
    "            log2, sqrt, None\n",
    "        \"\"\"\n",
    "\n",
    "        self.n = n\n",
    "        self.sigma = sigma\n",
    "        self.classes = classes\n",
    "\n",
    "        self.L = L\n",
    "        self.y = y\n",
    "        self.U = U\n",
    "\n",
    "        #Used to re-train trees from scratch\n",
    "        self.U_pseudo_tags = ( np.ones(shape=((self.U.shape[0]), self.n), dtype=int) * -1 )\n",
    "\n",
    "        self.mask_L = np.zeros(shape=((self.L.shape[0]), self.n), dtype=int, order='C') \n",
    "        self.mask_U = np.zeros(shape=((self.U.shape[0]), self.n), dtype=int, order='C')\n",
    "\n",
    "        self.ensemble = self.create_trees(max_features) \n",
    "\n",
    "        #self.fit()\n",
    "\n",
    "\n",
    "    def create_trees(self, max_features) -> dict:\n",
    "        \"\"\"Generates a dict -> {key: int, value: Tree}\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_features: number of features to consider \n",
    "                      when looking for the best split\n",
    "                      'sqrt', 'log2', None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            dict containing the trees of co-forest\n",
    "        \"\"\"\n",
    "\n",
    "        ensemble = {}\n",
    "\n",
    "        for i in range(self.n):\n",
    "\n",
    "            #IMPORTANT: bootstrapping only 50% of L size (few data to let oob error work)\n",
    "            rand_rows = np.random.choice(a = np.arange(start=0, stop=self.L.shape[0]), replace = True, size=(int(0.5*self.L.shape[0])) )\n",
    "            self.mask_L[rand_rows, i] = 1\n",
    "            h = DecisionTreeClassifier(max_features=max_features, random_state=1)\n",
    "            ensemble[i] = h.fit(self.L[rand_rows, :], self.y[rand_rows])\n",
    "\n",
    "        return ensemble\n",
    "\n",
    "\n",
    "    def get_rows_training(self, i: int, both=True) -> np.array:\n",
    "        \"\"\"Returns X used for training a tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i: int\n",
    "            index of the tree\n",
    "        both: bool\n",
    "            True if L union U is desired, False if L.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            array containing data used during training.\n",
    "        \"\"\"\n",
    "        if both:\n",
    "            return np.concatenate( (self.L[self.mask_L[:, i] == 1], self.U[self.mask_U[:, i] == 1]) )\n",
    "        else:\n",
    "            return self.L[self.mask_L[:, i] == 1]\n",
    "\n",
    "\n",
    "    def get_tags_training(self, i, both = True):\n",
    "        \"\"\"Returns y used for training a tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i: int\n",
    "            index of the tree\n",
    "        both: bool\n",
    "            True if L union U is desired, False if L.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            array containing tags of the data used.\n",
    "        \"\"\"\n",
    "        if both:\n",
    "            U_tags_i = self.U_pseudo_tags[:, i]\n",
    "            return np.concatenate( (self.y[self.mask_L[:, i] == 1], U_tags_i[self.mask_U[:, i] == 1]) )\n",
    "        \n",
    "        else:\n",
    "            return self.y[self.mask_L[:, i] == 1]\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Fits the ensemble using both labeled and\n",
    "        pseudo-labeled data.\n",
    "        \"\"\"\n",
    "\n",
    "        previous_e = [0.5 for i in range(self.n)]\n",
    "        previous_W = [min(0.1*len(self.L), 100) for i in range(self.n)]\n",
    "        new_data = True\n",
    "        t = 0\n",
    "        \n",
    "        while new_data:\n",
    "\n",
    "            t += 1\n",
    "            # Previous pseudo-labels are discarded on each iteration.\n",
    "            self.mask_U = np.zeros(shape=((self.U.shape[0]), self.n), dtype=int, order='C')\n",
    "            tree_changes = [False for i in range(self.n)]\n",
    "\n",
    "            for i, hi in self.ensemble.items():\n",
    "\n",
    "                e = self.concomitant_oob_error(hi)\n",
    "\n",
    "                if e < previous_e[i]: #incluir tratamiento de nan\n",
    "\n",
    "                    e = (lambda x: 0.000001 if x == 0 else x)(e)\n",
    "                    Wmax = ((previous_e[i]*previous_W[i])/e) #Tiende a infinito si el error es 0\n",
    "                    U_subsampled = self.subsample(hi, min(Wmax, 100))\n",
    "\n",
    "                    W = 0\n",
    "\n",
    "                    for u in U_subsampled:\n",
    "                        concomitant_confidence, selected_class = self.concomitant_confidence(hi, self.U[u, :])\n",
    "\n",
    "                        if concomitant_confidence > self.sigma:\n",
    "                            tree_changes[i] = True\n",
    "                            self.mask_U[u, i] = 1\n",
    "                            self.U_pseudo_tags[u, i] = selected_class\n",
    "                            W += concomitant_confidence\n",
    "\n",
    "                    previous_W[i] = W\n",
    "\n",
    "                previous_e[i] = (lambda x: 0 if x == 0.000001 else x)(e)\n",
    "\n",
    "            new_data = self.retrain_ensemble(np.array(tree_changes))\n",
    "\n",
    "\n",
    "    def retrain_ensemble(self, tree_changes: np.array) -> bool:\n",
    "        \"\"\"Retrains from scratch those trees that have \n",
    "        received new data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree_changes : boolean numpy array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if one or more trees have chanded, False if not.\n",
    "        \"\"\"\n",
    "\n",
    "        if tree_changes.sum() == 0:\n",
    "            return False\n",
    "\n",
    "        for i in np.fromiter(self.ensemble.keys(), dtype=int)[tree_changes]:\n",
    "            self.ensemble[i] = self.ensemble[i].fit(self.get_rows_training(i), self.get_tags_training(i))\n",
    "        \n",
    "        return True\n",
    "        \n",
    "\n",
    "    def subsample(self, hi: DecisionTreeClassifier, Wmax: float) -> np.array:\n",
    "        \"\"\"Samples from U uniformly at random until \n",
    "        the sum of the sample weights reaches Wmax.\n",
    "        Bootstraping is applied.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hi : DecisionTreeClassifier\n",
    "        Wmax: float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            Array containing the index of the chosen\n",
    "            samples from U\n",
    "        \"\"\"\n",
    "\n",
    "        W = 0\n",
    "        U_subsampled = []\n",
    "\n",
    "        while (W < Wmax):\n",
    "\n",
    "            rand_row = np.random.choice(a = np.arange(start=0, stop=self.U.shape[0]))\n",
    "            W += self.concomitant_confidence(hi, self.U[rand_row, :])[0]\n",
    "            U_subsampled.append(rand_row)\n",
    "\n",
    "        return np.array(U_subsampled)\n",
    "\n",
    "        \n",
    "    def concomitant_oob_error(self, hi: DecisionTreeClassifier) -> float:\n",
    "        \"\"\"Calculates the Out of Bag Error of the concomitant \n",
    "        ensemble of hi for the whole labeled data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hi : DecisionTreeClassifier\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            OOBE if trees voted, nan if not\n",
    "        \"\"\"\n",
    "\n",
    "        errors = []\n",
    "\n",
    "        for sample, tag in zip(self.L, self.y):\n",
    "            \n",
    "            n_votes = n_hits = 0 \n",
    "\n",
    "            for i, tree in self.ensemble.items():\n",
    "                if tree != hi and sample not in self.get_rows_training(i, both = False):\n",
    "\n",
    "                    if tree.predict([sample])[0] == tag:\n",
    "                        n_hits += 1\n",
    "                    n_votes +=1\n",
    "\n",
    "            if (n_votes > 0):\n",
    "                errors.append(1 - (n_hits/n_votes))\n",
    "\n",
    "        return np.mean(a=errors)\n",
    "\n",
    "    def concomitant_confidence(self, hi: DecisionTreeClassifier, sample: np.array) -> tuple:\n",
    "        \"\"\"Calculates the number of coincidences during\n",
    "        prediction of the hi concomitant ensemble for a\n",
    "        data sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hi : DecisionTreeClassifier\n",
    "        sample: sample's features array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple (float, int)\n",
    "            float: confidence for the sample\n",
    "            int: most agreed class\n",
    "        \"\"\"\n",
    "\n",
    "        count = { **dict.fromkeys([i for i in self.classes], 0)} \n",
    "        for i in (tree.predict([sample])[0] for tree in self.ensemble.values() if tree != hi):\n",
    "            count[i]+= 1\n",
    "\n",
    "        max_agreement = max(count.values())\n",
    "        most_agreed_class = list(count.values()).index(max_agreement)\n",
    "        return max_agreement/(len(self.ensemble) -1), most_agreed_class\n",
    "\n",
    "\n",
    "    def single_predict(self, sample: np.array): \n",
    "        \"\"\"Returns the class predicted by coforest\n",
    "        for a given sample. Majority voting is used.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample: np_array\n",
    "            sample to predict\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array:\n",
    "            label predicted by coforest.\n",
    "        \"\"\"\n",
    "\n",
    "        count = { **dict.fromkeys([i for i in self.classes], 0)} \n",
    "        for i in (tree.predict([sample])[0] for tree in self.ensemble.values()):\n",
    "            count[i]+= 1\n",
    "\n",
    "        max_agreement = max(count.values())\n",
    "        return self.classes[list(count.values()).index(max_agreement)]\n",
    "\n",
    "\n",
    "    def predict(self, samples: np.array) -> np.array:\n",
    "        \"\"\"Returns the labels predicted by the coforest\n",
    "        for a given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples: np_array\n",
    "            samples to predict\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array:\n",
    "            labels predicted by the coforest.\n",
    "        \"\"\"\n",
    "        \n",
    "        samples = (lambda x: np.expand_dims(x, axis=0) if x.ndim == 1 else x)(samples)\n",
    "        return np.array([self.single_predict(sample) for sample in samples])\n",
    "\n",
    "\n",
    "    def score(self, X_test: np.array, y_test: np.array) -> float:\n",
    "        \"\"\"Calculates the number of hits by coforest\n",
    "        given a training set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test: np_array\n",
    "            Samples used during testing\n",
    "        y_test: np_array\n",
    "            Samples' tags\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            percentage of hits.\n",
    "        \"\"\"\n",
    "        y_predictions = self.predict(X_test)\n",
    "        return np.count_nonzero(y_predictions==y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(iris.data), np.array(iris.target), test_size=0.2)\n",
    "L_train, U_train, Ly_train, Uy_train = train_test_split(X_train, y_train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9333333333333333\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "coforest = Co_Forest(L_train, Ly_train, U_train, 10, 0.75, [0,1,2])\n",
    "\n",
    "print(coforest.concomitant_oob_error(coforest.ensemble[1]))\n",
    "\n",
    "print(coforest.score(X_test, y_test))\n",
    "coforest.fit()\n",
    "print(coforest.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5061e2ebcf242305dcdb45d871ef5cd4ba433365f314bc2418d41c77ce076e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
